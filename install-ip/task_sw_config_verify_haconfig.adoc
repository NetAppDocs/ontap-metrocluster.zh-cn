---
permalink: install-ip/task_sw_config_verify_haconfig.html 
sidebar: sidebar 
keywords: metrocluster, ha-config, mccip, haconfig, verify, high-availability 
summary: 在出厂时未预配置的 MetroCluster IP 配置中，您必须验证控制器和机箱组件的 ha-config 状态是否设置为 mccip ，以便它们可以正常启动。对于从工厂收到的系统，此值是预配置的，您无需对其进行验证。 
---
= 验证组件的 ha-config 状态
:allow-uri-read: 


[role="lead"]
在出厂时未预配置的 MetroCluster IP 配置中，您必须验证控制器和机箱组件的 ha-config 状态是否设置为 `mCCIP` ，以便它们可以正常启动。对于从工厂收到的系统，此值是预配置的，您无需对其进行验证。

系统必须处于维护模式。

.步骤
. 显示控制器模块和机箱的 HA 状态：
+
`ha-config show`

+
控制器模块和机箱应显示值 `mCCIP` 。

. 如果显示的控制器系统状态不是 " `mCCIP` " ，请设置控制器的 HA 状态：
+
`ha-config modify controller mccip`

. 如果显示的机箱系统状态不是 " `mCCIP` " ，请设置机箱的 HA 状态：
+
`ha-config modify chassis mccip`

. 对 MetroCluster 配置中的每个节点重复上述步骤。




== 还原控制器模块上的系统默认值

[role="lead"]
重置和还原控制器模块上的默认值。

. 在 LOADER 提示符处，将环境变量返回到其默认设置： `set-defaults`
. 将节点启动至启动菜单： `boot_ontap menu`
+
运行此命令后，请等待，直到显示启动菜单为止。

. 清除节点配置：
+
--
** 如果您使用的系统配置了 ADP ，请从启动菜单中选择选项 `9a` ，并在出现提示时回答 `yes` 。
+

NOTE: 此过程会造成中断。

+
以下屏幕将显示启动菜单提示符：

+
[listing]
----

Please choose one of the following:

    (1) Normal Boot.
    (2) Boot without /etc/rc.
    (3) Change password.
    (4) Clean configuration and initialize all disks.
    (5) Maintenance mode boot.
    (6) Update flash from backup config.
    (7) Install new software first.
    (8) Reboot node.
    (9) Configure Advanced Drive Partitioning.
    Selection (1-9)?  9a
########## WARNING ##########

    This is a disruptive operation and will result in the
    loss of all filesystem data. Before proceeding further,
    make sure that:
    1) This option (9a) has been executed or will be executed
    on the HA partner node, prior to reinitializing either
    system in the HA-pair.
    2) The HA partner node is currently in a halted state or
    at the LOADER prompt.


    Do you still want to continue (yes/no)? yes
----


--
+
** 如果您的系统未配置 ADP ，请在启动菜单提示符处键入 `wipeconfig` ，然后按 Enter 键。
+
以下屏幕将显示启动菜单提示符：

+
[listing]
----

Please choose one of the following:

    (1) Normal Boot.
    (2) Boot without /etc/rc.
    (3) Change password.
    (4) Clean configuration and initialize all disks.
    (5) Maintenance mode boot.
    (6) Update flash from backup config.
    (7) Install new software first.
    (8) Reboot node.
    (9) Configure Advanced Drive Partitioning.
    Selection (1-9)?  wipeconfig
This option deletes critical system configuration, including cluster membership.
Warning: do not run this option on a HA node that has been taken over.
Are you sure you want to continue?: yes
Rebooting to finish wipeconfig request.
----






== 手动将驱动器分配给池 0

如果您未收到出厂时预配置的系统，则可能需要手动分配池 0 驱动器。根据平台型号以及系统是否使用 ADP ，您必须为 MetroCluster IP 配置中的每个节点手动将驱动器分配到池 0 。您使用的操作步骤取决于所使用的 ONTAP 版本。

* <<man_assign_pool_0_9_4,手动为池 0 分配驱动器（ ONTAP 9.4 及更高版本）>>
* <<man_assign_pool_0_9_3,手动为池 0 分配驱动器（ ONTAP 9.3 ）>>




=== 手动为池 0 分配驱动器（ ONTAP 9.4 及更高版本）

如果系统在出厂时尚未进行预配置，并且不满足自动驱动器分配的要求，则必须手动分配池 0 驱动器。

此操作步骤适用场景配置运行 ONTAP 9.4 或更高版本。

要确定您的系统是否需要手动分配磁盘，应查看 link:concept_considerations_drive_assignment.html["ONTAP 9.4 及更高版本中的自动驱动器分配和 ADP 系统注意事项"]。

您可以在维护模式下执行这些步骤。必须对配置中的每个节点执行操作步骤。

本节中的示例基于以下假设：

* node_A_1 和 node_A_2 在以下位置拥有驱动器：
+
** site_A-shelf_1 （本地）
** site_B-shelf_2 （远程）


* node_B_1 和 node_B_2 在以下位置拥有驱动器：
+
** site_B-shelf_1 （本地）
** site_A-shelf_2 （远程）




.步骤
. 显示启动菜单：
+
`boot_ontap 菜单`

. 选择选项 "9a" 。
+
以下屏幕将显示启动菜单提示符：

+
[listing]
----

Please choose one of the following:

    (1) Normal Boot.
    (2) Boot without /etc/rc.
    (3) Change password.
    (4) Clean configuration and initialize all disks.
    (5) Maintenance mode boot.
    (6) Update flash from backup config.
    (7) Install new software first.
    (8) Reboot node.
    (9) Configure Advanced Drive Partitioning.
    Selection (1-9)?  9a
########## WARNING ##########

    This is a disruptive operation and will result in the
    loss of all filesystem data. Before proceeding further,
    make sure that:
    1) This option (9a) has been executed or will be executed
    on the HA partner node (and DR/DR-AUX partner nodes if
    applicable), prior to reinitializing any system in the
    HA-pair (or MetroCluster setup).
    2) The HA partner node (and DR/DR-AUX partner nodes if
    applicable) is currently waiting at the boot menu.

    Do you still want to continue (yes/no)? yes
----
. 节点重新启动时，在系统提示显示启动菜单时按 Ctrl-C ，然后选择 * 维护模式启动 * 选项。
. 在维护模式下，手动为节点上的本地聚合分配驱动器：
+
`ddisk assign _disk-id_ -p 0 -s _local-node-sysid_`

+
应对称分配驱动器，以便每个节点具有相同数量的驱动器。以下步骤适用于每个站点具有两个存储架的配置。

+
.. 配置 node_A_1 时，从 site_A-shelf_1 手动将插槽 0 到 11 的驱动器分配给节点 A1 的 pool0 。
.. 配置 node_A_2 时，手动将插槽 12 中的驱动器分配给 site_A-shelf_1 中节点 A2 的 pool0 。
.. 配置 node_B_1 时，手动将插槽 0 到 11 的驱动器分配给 site_B-shelf_1 中节点 B1 的 pool0 。
.. 配置 node_B_2 时，手动将插槽 12 中的驱动器分配给 site_B-shelf_1 中节点 B2 的 pool0 。


. 退出维护模式：
+
`halt`

. 显示启动菜单：
+
`boot_ontap 菜单`

. 从启动菜单中选择选项 4 ，然后让系统启动。
. 在 MetroCluster IP 配置中的其他节点上重复上述步骤。
. 继续执行 link:concept_configure_the_mcc_software_in_ontap.html#setting-up-ontap["设置 ONTAP"]。




=== 手动为池 0 分配驱动器（ ONTAP 9.3 ）

如果每个节点至少有两个磁盘架，则可以使用 ONTAP 的自动分配功能自动分配本地（池 0 ）磁盘。

当节点处于维护模式时，您必须先将相应磁盘架上的单个磁盘分配给池 0 。然后， ONTAP 会自动将磁盘架上的其余磁盘分配到同一个池。从工厂收到的系统不需要执行此任务，这些系统的池 0 包含预配置的根聚合。

此操作步骤适用场景配置运行 ONTAP 9.3 。

如果您是从工厂收到 MetroCluster 配置的，则不需要此操作步骤。出厂时，节点配置了池 0 磁盘和根聚合。

只有当每个节点至少有两个磁盘架时，才可以使用此操作步骤，从而可以在磁盘架级别自动分配磁盘。如果不能使用磁盘架级别的自动分配，则必须手动分配本地磁盘，以便每个节点都有一个本地磁盘池（池 0 ）。

必须在维护模式下执行这些步骤。

本节中的示例假定使用以下磁盘架：

* node_A_1 拥有以下位置的磁盘：
+
** site_A-shelf_1 （本地）
** site_B-shelf_2 （远程）


* node_A_2 连接到：
+
** site_A-shelf_3 （本地）
** site_B-shelf_4 （远程）


* node_B_1 连接到：
+
** site_B-shelf_1 （本地）
** site_A-shelf_2 （远程）


* node_B_2 连接到：
+
** site_B-shelf_3 （本地）
** site_A-shelf_4 （远程）




.步骤
. 在每个节点上手动为根聚合分配一个磁盘：
+
`ddisk assign _disk-id_ -p 0 -s _local-node-sysid_`

+
通过手动分配这些磁盘， ONTAP 自动分配功能可以分配每个磁盘架上的其余磁盘。

+
.. 在 node_A_1 上，手动将一个磁盘从本地 site_A-shelf_1 分配到池 0 。
.. 在 node_A_2 上，手动将一个磁盘从 local site_A-shelf_3 分配到池 0 。
.. 在 node_B_1 上，手动将一个磁盘从 local site_B-shelf_1 分配到池 0 。
.. 在 node_B_2 上，手动将一个磁盘从 local site_B-shelf_3 分配给池 0 。


. 使用启动菜单上的选项 "`4` " 启动站点 A 上的每个节点：
+
您应先在节点上完成此步骤，然后再继续下一个节点。

+
.. 退出维护模式：
+
`halt`

.. 显示启动菜单：
+
`boot_ontap 菜单`

.. 从启动菜单中选择选项 "`4` " 并继续。


. 使用启动菜单上的选项 "`4` " 启动站点 B 上的每个节点：
+
您应先在节点上完成此步骤，然后再继续下一个节点。

+
.. 退出维护模式：
+
`halt`

.. 显示启动菜单：
+
`boot_ontap 菜单`

.. 从启动菜单中选择选项 4 并继续。






== 设置 ONTAP

启动每个节点后，系统将提示您执行基本节点和集群配置。配置集群后，您可以返回到 ONTAP 命令行界面以创建聚合并创建 MetroCluster 配置。

.开始之前
* 您必须已为 MetroCluster 配置布线。
* 您不能事先配置服务处理器。


如果需要通过网络启动新控制器，请参见 link:../upgrade/task_upgrade_controllers_in_a_four_node_ip_mcc_us_switchover_and_switchback_mcc_ip.html#netbooting-the-new-controllers["通过网络启动新控制器模块"]。

必须对 MetroCluster 配置中的两个集群执行此任务。

.步骤
. 如果尚未启动本地站点上的每个节点，请将其启动并让其完全启动。
+
如果系统处于维护模式，则需要使用问题描述 halt 命令退出维护模式，然后使用问题描述 `boot_ontap` 命令启动系统并进入集群设置。

. 在每个集群中的第一个节点上，按照提示继续配置集群
+
.. 按照系统提供的说明启用 AutoSupport 工具。
+
输出应类似于以下内容：

+
[listing]
----
Welcome to the cluster setup wizard.

    You can enter the following commands at any time:
    "help" or "?" - if you want to have a question clarified,
    "back" - if you want to change previously answered questions, and
    "exit" or "quit" - if you want to quit the cluster setup wizard.
    Any changes you made before quitting will be saved.

    You can return to cluster setup at any time by typing "cluster setup".
    To accept a default or omit a question, do not enter a value.

    This system will send event messages and periodic reports to NetApp Technical
    Support. To disable this feature, enter
    autosupport modify -support disable
    within 24 hours.

    Enabling AutoSupport can significantly speed problem determination and
    resolution should a problem occur on your system.
    For further information on AutoSupport, see:
    http://support.netapp.com/autosupport/

    Type yes to confirm and continue {yes}: yes

.
.
.
----
.. 通过响应提示来配置节点管理接口。
+
这些提示类似于以下内容：

+
[listing]
----
Enter the node management interface port [e0M]:
Enter the node management interface IP address: 172.17.8.229
Enter the node management interface netmask: 255.255.254.0
Enter the node management interface default gateway: 172.17.8.1
A node management interface on port e0M with IP address 172.17.8.229 has been created.
----
.. 响应提示创建集群。
+
这些提示类似于以下内容：

+
[listing]
----
Do you want to create a new cluster or join an existing cluster? {create, join}:
create


Do you intend for this node to be used as a single node cluster? {yes, no} [no]:
no

Existing cluster interface configuration found:

Port MTU IP Netmask
e0a 1500 169.254.18.124 255.255.0.0
e1a 1500 169.254.184.44 255.255.0.0

Do you want to use this configuration? {yes, no} [yes]: no

System Defaults:
Private cluster network ports [e0a,e1a].
Cluster port MTU values will be set to 9000.
Cluster interface IP addresses will be automatically generated.

Do you want to use these defaults? {yes, no} [yes]: no

Enter the cluster administrator's (username "admin") password:

Retype the password:


Step 1 of 5: Create a Cluster
You can type "back", "exit", or "help" at any question.

List the private cluster network ports [e0a,e1a]:
Enter the cluster ports' MTU size [9000]:
Enter the cluster network netmask [255.255.0.0]: 255.255.254.0
Enter the cluster interface IP address for port e0a: 172.17.10.228
Enter the cluster interface IP address for port e1a: 172.17.10.229
Enter the cluster name: cluster_A

Creating cluster cluster_A

Starting cluster support services ...

Cluster cluster_A has been created.
----
.. 添加许可证，设置集群管理 SVM ，并通过响应提示输入 DNS 信息。
+
这些提示类似于以下内容：

+
[listing]
----
Step 2 of 5: Add Feature License Keys
You can type "back", "exit", or "help" at any question.

Enter an additional license key []:


Step 3 of 5: Set Up a Vserver for Cluster Administration
You can type "back", "exit", or "help" at any question.


Enter the cluster management interface port [e3a]:
Enter the cluster management interface IP address: 172.17.12.153
Enter the cluster management interface netmask: 255.255.252.0
Enter the cluster management interface default gateway: 172.17.12.1

A cluster management interface on port e3a with IP address 172.17.12.153 has been created. You can use this address to connect to and manage the cluster.

Enter the DNS domain names: lab.netapp.com
Enter the name server IP addresses: 172.19.2.30
DNS lookup for the admin Vserver will use the lab.netapp.com domain.

Step 4 of 5: Configure Storage Failover (SFO)
You can type "back", "exit", or "help" at any question.


SFO will be enabled when the partner joins the cluster.


Step 5 of 5: Set Up the Node
You can type "back", "exit", or "help" at any question.

Where is the controller located []: svl
----
.. 启用存储故障转移并通过响应提示来设置节点。
+
这些提示类似于以下内容：

+
[listing]
----
Step 4 of 5: Configure Storage Failover (SFO)
You can type "back", "exit", or "help" at any question.


SFO will be enabled when the partner joins the cluster.


Step 5 of 5: Set Up the Node
You can type "back", "exit", or "help" at any question.

Where is the controller located []: site_A
----
.. 完成节点配置，但不创建数据聚合。
+
您可以使用 ONTAP 系统管理器将 Web 浏览器指向集群管理 IP 地址 (https://172.17.12.153)[]。

+
https://docs.netapp.com/ontap-9/topic/com.netapp.doc.onc-sm-help/GUID-DF04A607-30B0-4B98-99C8-CB065C64E670.html["使用 System Manager 进行集群管理（ 9.0 到 9.6 版）"^]

+
https://docs.netapp.com/us-en/ontap/index.html["ONTAP System Manager （ 9.7 及更高版本）"^]



. 按照提示启动下一个控制器并将其加入集群。
. 确认节点已配置为高可用性模式：
+
`s存储故障转移 show -fields mode`

+
如果没有，则必须在每个节点上配置 HA 模式，然后重新启动节点：

+
`storage failover modify -mode ha -node _localhost_`

+
此命令可配置高可用性模式，但不会启用存储故障转移。如果稍后配置 MetroCluster 配置，则会自动启用存储故障转移。

. 确认已将四个端口配置为集群互连：
+
`network port show`

+
此时未配置 MetroCluster IP 接口，并且此接口不会显示在命令输出中。

+
以下示例显示了 node_A_1 上的两个集群端口：

+
[listing]
----
cluster_A::*> network port show -role cluster



Node: node_A_1

                                                                       Ignore

                                                  Speed(Mbps) Health   Health

Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status

--------- ------------ ---------------- ---- ---- ----------- -------- ------

e4a       Cluster      Cluster          up   9000  auto/40000 healthy  false

e4e       Cluster      Cluster          up   9000  auto/40000 healthy  false


Node: node_A_2

                                                                       Ignore

                                                  Speed(Mbps) Health   Health

Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status

--------- ------------ ---------------- ---- ---- ----------- -------- ------

e4a       Cluster      Cluster          up   9000  auto/40000 healthy  false

e4e       Cluster      Cluster          up   9000  auto/40000 healthy  false


4 entries were displayed.
----
. 在配对集群上重复上述步骤。


返回到 ONTAP 命令行界面，并通过执行以下任务完成 MetroCluster 配置。



== 将集群配置为 MetroCluster 配置

您必须对集群建立对等关系，镜像根聚合，创建镜像数据聚合，然后问题描述命令以实施 MetroCluster 操作。



=== 禁用自动驱动器分配（如果在 ONTAP 9.4 中执行手动分配）

在 ONTAP 9.4 中，如果您的 MetroCluster IP 配置中每个站点的外部存储架少于四个，则必须在所有节点上禁用驱动器自动分配并手动分配驱动器。

在 ONTAP 9.5 及更高版本中不需要执行此任务。

此任务不适用于具有内部磁盘架且无外部磁盘架的 AFF A800 系统。

link:concept_considerations_drive_assignment.html["ONTAP 9.4 及更高版本中的自动驱动器分配和 ADP 系统注意事项"]

.步骤
. 禁用自动驱动器分配：
+
`s存储磁盘选项 modify -node node_name -autosassign off`

+
您需要在 MetroCluster IP 配置中的所有节点上问题描述此命令。





=== 验证池 0 驱动器的驱动器分配

您必须验证远程驱动器对节点可见且已正确分配。

自动分配取决于存储系统平台型号和驱动器架布置。

link:concept_considerations_drive_assignment.html["ONTAP 9.4 及更高版本中的自动驱动器分配和 ADP 系统注意事项"]

.步骤
. 验证是否自动分配池 0 驱动器：
+
`d展示`

+
以下示例显示了没有外部磁盘架的 AFF A800 系统的 cluster_A 输出。

+
四分之一（ 8 个驱动器）自动分配给 "node_A_1" ，四分之一自动分配给 "node_A_2" 。其余驱动器将是 "node_B_1 和 "node_B_2" 的远程（池 1 ）驱动器。

+
[listing]
----
cluster_A::*> disk show
                 Usable     Disk      Container           Container
Disk             Size       Shelf Bay Type    Type        Name      Owner
---------------- ---------- ----- --- ------- ----------- --------- --------
node_A_1:0n.12   1.75TB     0     12  SSD-NVM shared      aggr0     node_A_1
node_A_1:0n.13   1.75TB     0     13  SSD-NVM shared      aggr0     node_A_1
node_A_1:0n.14   1.75TB     0     14  SSD-NVM shared      aggr0     node_A_1
node_A_1:0n.15   1.75TB     0     15  SSD-NVM shared      aggr0     node_A_1
node_A_1:0n.16   1.75TB     0     16  SSD-NVM shared      aggr0     node_A_1
node_A_1:0n.17   1.75TB     0     17  SSD-NVM shared      aggr0     node_A_1
node_A_1:0n.18   1.75TB     0     18  SSD-NVM shared      aggr0     node_A_1
node_A_1:0n.19   1.75TB     0     19  SSD-NVM shared      -         node_A_1
node_A_2:0n.0    1.75TB     0     0   SSD-NVM shared      aggr0_node_A_2_0 node_A_2
node_A_2:0n.1    1.75TB     0     1   SSD-NVM shared      aggr0_node_A_2_0 node_A_2
node_A_2:0n.2    1.75TB     0     2   SSD-NVM shared      aggr0_node_A_2_0 node_A_2
node_A_2:0n.3    1.75TB     0     3   SSD-NVM shared      aggr0_node_A_2_0 node_A_2
node_A_2:0n.4    1.75TB     0     4   SSD-NVM shared      aggr0_node_A_2_0 node_A_2
node_A_2:0n.5    1.75TB     0     5   SSD-NVM shared      aggr0_node_A_2_0 node_A_2
node_A_2:0n.6    1.75TB     0     6   SSD-NVM shared      aggr0_node_A_2_0 node_A_2
node_A_2:0n.7    1.75TB     0     7   SSD-NVM shared      -         node_A_2
node_A_2:0n.24   -          0     24  SSD-NVM unassigned  -         -
node_A_2:0n.25   -          0     25  SSD-NVM unassigned  -         -
node_A_2:0n.26   -          0     26  SSD-NVM unassigned  -         -
node_A_2:0n.27   -          0     27  SSD-NVM unassigned  -         -
node_A_2:0n.28   -          0     28  SSD-NVM unassigned  -         -
node_A_2:0n.29   -          0     29  SSD-NVM unassigned  -         -
node_A_2:0n.30   -          0     30  SSD-NVM unassigned  -         -
node_A_2:0n.31   -          0     31  SSD-NVM unassigned  -         -
node_A_2:0n.36   -          0     36  SSD-NVM unassigned  -         -
node_A_2:0n.37   -          0     37  SSD-NVM unassigned  -         -
node_A_2:0n.38   -          0     38  SSD-NVM unassigned  -         -
node_A_2:0n.39   -          0     39  SSD-NVM unassigned  -         -
node_A_2:0n.40   -          0     40  SSD-NVM unassigned  -         -
node_A_2:0n.41   -          0     41  SSD-NVM unassigned  -         -
node_A_2:0n.42   -          0     42  SSD-NVM unassigned  -         -
node_A_2:0n.43   -          0     43  SSD-NVM unassigned  -         -
32 entries were displayed.
----
+
以下示例显示了 cluster_B 输出：

+
[listing]
----
cluster_B::> disk show
                 Usable     Disk              Container   Container
Disk             Size       Shelf Bay Type    Type        Name      Owner
---------------- ---------- ----- --- ------- ----------- --------- --------

Info: This cluster has partitioned disks. To get a complete list of spare disk
capacity use "storage aggregate show-spare-disks".
node_B_1:0n.12   1.75TB     0     12  SSD-NVM shared      aggr0     node_B_1
node_B_1:0n.13   1.75TB     0     13  SSD-NVM shared      aggr0     node_B_1
node_B_1:0n.14   1.75TB     0     14  SSD-NVM shared      aggr0     node_B_1
node_B_1:0n.15   1.75TB     0     15  SSD-NVM shared      aggr0     node_B_1
node_B_1:0n.16   1.75TB     0     16  SSD-NVM shared      aggr0     node_B_1
node_B_1:0n.17   1.75TB     0     17  SSD-NVM shared      aggr0     node_B_1
node_B_1:0n.18   1.75TB     0     18  SSD-NVM shared      aggr0     node_B_1
node_B_1:0n.19   1.75TB     0     19  SSD-NVM shared      -         node_B_1
node_B_2:0n.0    1.75TB     0     0   SSD-NVM shared      aggr0_node_B_1_0 node_B_2
node_B_2:0n.1    1.75TB     0     1   SSD-NVM shared      aggr0_node_B_1_0 node_B_2
node_B_2:0n.2    1.75TB     0     2   SSD-NVM shared      aggr0_node_B_1_0 node_B_2
node_B_2:0n.3    1.75TB     0     3   SSD-NVM shared      aggr0_node_B_1_0 node_B_2
node_B_2:0n.4    1.75TB     0     4   SSD-NVM shared      aggr0_node_B_1_0 node_B_2
node_B_2:0n.5    1.75TB     0     5   SSD-NVM shared      aggr0_node_B_1_0 node_B_2
node_B_2:0n.6    1.75TB     0     6   SSD-NVM shared      aggr0_node_B_1_0 node_B_2
node_B_2:0n.7    1.75TB     0     7   SSD-NVM shared      -         node_B_2
node_B_2:0n.24   -          0     24  SSD-NVM unassigned  -         -
node_B_2:0n.25   -          0     25  SSD-NVM unassigned  -         -
node_B_2:0n.26   -          0     26  SSD-NVM unassigned  -         -
node_B_2:0n.27   -          0     27  SSD-NVM unassigned  -         -
node_B_2:0n.28   -          0     28  SSD-NVM unassigned  -         -
node_B_2:0n.29   -          0     29  SSD-NVM unassigned  -         -
node_B_2:0n.30   -          0     30  SSD-NVM unassigned  -         -
node_B_2:0n.31   -          0     31  SSD-NVM unassigned  -         -
node_B_2:0n.36   -          0     36  SSD-NVM unassigned  -         -
node_B_2:0n.37   -          0     37  SSD-NVM unassigned  -         -
node_B_2:0n.38   -          0     38  SSD-NVM unassigned  -         -
node_B_2:0n.39   -          0     39  SSD-NVM unassigned  -         -
node_B_2:0n.40   -          0     40  SSD-NVM unassigned  -         -
node_B_2:0n.41   -          0     41  SSD-NVM unassigned  -         -
node_B_2:0n.42   -          0     42  SSD-NVM unassigned  -         -
node_B_2:0n.43   -          0     43  SSD-NVM unassigned  -         -
32 entries were displayed.

cluster_B::>
----




=== 为集群建立对等关系

MetroCluster 配置中的集群必须处于对等关系中，以便它们可以彼此通信并执行对 MetroCluster 灾难恢复至关重要的数据镜像。

http://docs.netapp.com/ontap-9/topic/com.netapp.doc.exp-clus-peer/home.html["集群和 SVM 对等快速配置"^]

link:concept_considerations_peering.html#considerations-when-using-dedicated-ports["使用专用端口时的注意事项"]

link:concept_considerations_peering.html#considerations-when-sharing-data-ports["共享数据端口时的注意事项"]



=== 为集群对等配置集群间 LIF

您必须在用于 MetroCluster 配对集群之间通信的端口上创建集群间 LIF 。您可以使用专用端口或也具有数据流量的端口。



==== 在专用端口上配置集群间 LIF

您可以在专用端口上配置集群间 LIF 。这样做通常会增加复制流量的可用带宽。

.步骤
. 列出集群中的端口：
+
`network port show`

+
有关完整的命令语法，请参见手册页。

+
以下示例显示了 cluster01 中的网络端口：

+
[listing]
----

cluster01::> network port show
                                                             Speed (Mbps)
Node   Port      IPspace      Broadcast Domain Link   MTU    Admin/Oper
------ --------- ------------ ---------------- ----- ------- ------------
cluster01-01
       e0a       Cluster      Cluster          up     1500   auto/1000
       e0b       Cluster      Cluster          up     1500   auto/1000
       e0c       Default      Default          up     1500   auto/1000
       e0d       Default      Default          up     1500   auto/1000
       e0e       Default      Default          up     1500   auto/1000
       e0f       Default      Default          up     1500   auto/1000
cluster01-02
       e0a       Cluster      Cluster          up     1500   auto/1000
       e0b       Cluster      Cluster          up     1500   auto/1000
       e0c       Default      Default          up     1500   auto/1000
       e0d       Default      Default          up     1500   auto/1000
       e0e       Default      Default          up     1500   auto/1000
       e0f       Default      Default          up     1500   auto/1000
----
. 确定哪些端口可专用于集群间通信：
+
`network interface show -fields home-port ， curr-port`

+
有关完整的命令语法，请参见手册页。

+
以下示例显示未为端口 "`e0e` " 和 "`e0f` " 分配 LIF ：

+
[listing]
----

cluster01::> network interface show -fields home-port,curr-port
vserver lif                  home-port curr-port
------- -------------------- --------- ---------
Cluster cluster01-01_clus1   e0a       e0a
Cluster cluster01-01_clus2   e0b       e0b
Cluster cluster01-02_clus1   e0a       e0a
Cluster cluster01-02_clus2   e0b       e0b
cluster01
        cluster_mgmt         e0c       e0c
cluster01
        cluster01-01_mgmt1   e0c       e0c
cluster01
        cluster01-02_mgmt1   e0c       e0c
----
. 为专用端口创建故障转移组：
+
`network interface failover-groups create -vserver _system_svm_ -failover-group _failover_group_ -targets _physical_or_logical_ports_`

+
以下示例将端口 "`e0e` " 和 "`e0f` " 分配给系统 " `SVMcluster01` " 上的故障转移组 "`intercluster01` " ：

+
[listing]
----
cluster01::> network interface failover-groups create -vserver cluster01 -failover-group
intercluster01 -targets
cluster01-01:e0e,cluster01-01:e0f,cluster01-02:e0e,cluster01-02:e0f
----
. 验证是否已创建故障转移组：
+
`network interface failover-groups show`

+
有关完整的命令语法，请参见手册页。

+
[listing]
----
cluster01::> network interface failover-groups show
                                  Failover
Vserver          Group            Targets
---------------- ---------------- --------------------------------------------
Cluster
                 Cluster
                                  cluster01-01:e0a, cluster01-01:e0b,
                                  cluster01-02:e0a, cluster01-02:e0b
cluster01
                 Default
                                  cluster01-01:e0c, cluster01-01:e0d,
                                  cluster01-02:e0c, cluster01-02:e0d,
                                  cluster01-01:e0e, cluster01-01:e0f
                                  cluster01-02:e0e, cluster01-02:e0f
                 intercluster01
                                  cluster01-01:e0e, cluster01-01:e0f
                                  cluster01-02:e0e, cluster01-02:e0f
----
. 在系统 SVM 上创建集群间 LIF 并将其分配给故障转移组。
+
|===


| ONTAP 版本 | 命令 


 a| 
9.6 及更高版本
 a| 
`network interface create -vserver _system_svm_ -lif _LIF_name_ -service-policy default-intercluster -home-node _node_-home-port _port_ -address _port_ip_ -netmask _netmask_ -failover-group _failover_group_`



 a| 
9.5 及更早版本
 a| 
`network interface create -vserver _system_svm_ -lif _LIF_name_ -role intercluster -home-node _node_-home-port _port_ -address _port_ip_ -netmask _netmask_ -failover-group _failover_group_`

|===
+
有关完整的命令语法，请参见手册页。

+
以下示例将在故障转移组 "`intercluster01` " 中创建集群间 LIF "`cluster01_icl01` " 和 "`cluster01_icl02` " ：

+
[listing]
----
cluster01::> network interface create -vserver cluster01 -lif cluster01_icl01 -service-
policy default-intercluster -home-node cluster01-01 -home-port e0e -address 192.168.1.201
-netmask 255.255.255.0 -failover-group intercluster01

cluster01::> network interface create -vserver cluster01 -lif cluster01_icl02 -service-
policy default-intercluster -home-node cluster01-02 -home-port e0e -address 192.168.1.202
-netmask 255.255.255.0 -failover-group intercluster01
----
. 验证是否已创建集群间 LIF ：
+
|===


| * 在 ONTAP 9.6 及更高版本中： * 


 a| 
`network interface show -service-policy default-intercluster`



| * 在 ONTAP 9.5 及更早版本中： * 


 a| 
`network interface show -role intercluster`

|===
+
有关完整的命令语法，请参见手册页。

+
[listing]
----
cluster01::> network interface show -service-policy default-intercluster
            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
cluster01
            cluster01_icl01
                       up/up      192.168.1.201/24   cluster01-01  e0e     true
            cluster01_icl02
                       up/up      192.168.1.202/24   cluster01-02  e0f     true
----
. 验证集群间 LIF 是否冗余：
+
|===


| * 在 ONTAP 9.6 及更高版本中： * 


 a| 
`network interface show -service-policy default-intercluster -failover`



| * 在 ONTAP 9.5 及更早版本中： * 


 a| 
`network interface show -role intercluster -failover`

|===
+
有关完整的命令语法，请参见手册页。

+
以下示例显示 "e0e" 端口上的集群间 LIF"cluster01_icl01" 和 "cluster01_icl02" 将故障转移到 "e0f" 端口。

+
[listing]
----
cluster01::> network interface show -service-policy default-intercluster –failover
         Logical         Home                  Failover        Failover
Vserver  Interface       Node:Port             Policy          Group
-------- --------------- --------------------- --------------- --------
cluster01
         cluster01_icl01 cluster01-01:e0e   local-only      intercluster01
                            Failover Targets:  cluster01-01:e0e,
                                               cluster01-01:e0f
         cluster01_icl02 cluster01-02:e0e   local-only      intercluster01
                            Failover Targets:  cluster01-02:e0e,
                                               cluster01-02:e0f
----


link:concept_considerations_peering.html#considerations-when-using-dedicated-ports["使用专用端口时的注意事项"]



==== 在共享数据端口上配置集群间 LIF

您可以在与数据网络共享的端口上配置集群间 LIF 。这样可以减少集群间网络连接所需的端口数量。

. 列出集群中的端口：
+
`network port show`

+
有关完整的命令语法，请参见手册页。

+
以下示例显示了 cluster01 中的网络端口：

+
[listing]
----

cluster01::> network port show
                                                             Speed (Mbps)
Node   Port      IPspace      Broadcast Domain Link   MTU    Admin/Oper
------ --------- ------------ ---------------- ----- ------- ------------
cluster01-01
       e0a       Cluster      Cluster          up     1500   auto/1000
       e0b       Cluster      Cluster          up     1500   auto/1000
       e0c       Default      Default          up     1500   auto/1000
       e0d       Default      Default          up     1500   auto/1000
cluster01-02
       e0a       Cluster      Cluster          up     1500   auto/1000
       e0b       Cluster      Cluster          up     1500   auto/1000
       e0c       Default      Default          up     1500   auto/1000
       e0d       Default      Default          up     1500   auto/1000
----
. 在系统 SVM 上创建集群间 LIF ：
+
|===


| * 在 ONTAP 9.6 及更高版本中： * 


 a| 
`network interface create -vserver _system_svm_ -lif _LIF_name_ -service-policy default-intercluster -home-node _node_-home-port _port_ -address _port_ip_ -netmask _netmask_`



| * 在 ONTAP 9.5 及更早版本中： * 


 a| 
`network interface create -vserver _system_svm_ -lif _LIF_name_ -role intercluster -home-node _node_-home-port _port_ -address _port_ip_ -netmask _netmask_`

|===
+
有关完整的命令语法，请参见手册页。

+
以下示例将创建集群间 LIF "cluster01_icl01" 和 "cluster01_icl02" ：

+
[listing]
----

cluster01::> network interface create -vserver cluster01 -lif cluster01_icl01 -service-
policy default-intercluster -home-node cluster01-01 -home-port e0c -address 192.168.1.201
-netmask 255.255.255.0

cluster01::> network interface create -vserver cluster01 -lif cluster01_icl02 -service-
policy default-intercluster -home-node cluster01-02 -home-port e0c -address 192.168.1.202
-netmask 255.255.255.0
----
. 验证是否已创建集群间 LIF ：
+
|===


| * 在 ONTAP 9.6 及更高版本中： * 


 a| 
`network interface show -service-policy default-intercluster`



| * 在 ONTAP 9.5 及更早版本中： * 


 a| 
`network interface show -role intercluster`

|===
+
有关完整的命令语法，请参见手册页。

+
[listing]
----
cluster01::> network interface show -service-policy default-intercluster
            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
cluster01
            cluster01_icl01
                       up/up      192.168.1.201/24   cluster01-01  e0c     true
            cluster01_icl02
                       up/up      192.168.1.202/24   cluster01-02  e0c     true
----
. 验证集群间 LIF 是否冗余：
+
|===


| * 在 ONTAP 9.6 及更高版本中： * 


 a| 
`network interface show – service-policy default-intercluster -failover`



| * 在 ONTAP 9.5 及更早版本中： * 


 a| 
`network interface show -role intercluster -failover`

|===
+
有关完整的命令语法，请参见手册页。

+
以下示例显示 "`e0c` " 端口上的集群间 LIF"`cluster01_icl01` " 和 "`cluster01_icl02` " 将故障转移到 "`e0d` " 端口。

+
[listing]
----
cluster01::> network interface show -service-policy default-intercluster –failover
         Logical         Home                  Failover        Failover
Vserver  Interface       Node:Port             Policy          Group
-------- --------------- --------------------- --------------- --------
cluster01
         cluster01_icl01 cluster01-01:e0c   local-only      192.168.1.201/24
                            Failover Targets: cluster01-01:e0c,
                                              cluster01-01:e0d
         cluster01_icl02 cluster01-02:e0c   local-only      192.168.1.201/24
                            Failover Targets: cluster01-02:e0c,
                                              cluster01-02:e0d
----


link:concept_considerations_peering.html#considerations-when-sharing-data-ports["共享数据端口时的注意事项"]



=== 创建集群对等关系

您可以使用 cluster peer create 命令在本地和远程集群之间创建对等关系。创建对等关系后，您可以在远程集群上运行 cluster peer create ，以便向本地集群进行身份验证。

.开始之前
* 您必须已在要建立对等关系的集群中的每个节点上创建集群间 LIF 。
* 集群必须运行 ONTAP 9.3 或更高版本。


.步骤
. 在目标集群上，创建与源集群的对等关系：
+
`cluster peer create -generate-passphrase -offer-expiration _MM/DD/YYYY HH ： MM ： SS|1...7 天 |1...168 小时 _ -peer-Addrs _peer_LIF_IP_ -IPspace _IPspace _IPspace _`

+
如果同时指定 ` generate-passphrase` 和 ` -peer-addrs` ，则只有在 ` -peer-addrs` 中指定了集群间 LIF 的集群才能使用生成的密码。

+
如果您不使用自定义 IP 空间，则可以忽略 ` -ipspace` 选项。有关完整的命令语法，请参见手册页。

+
以下示例将在未指定的远程集群上创建集群对等关系：

+
[listing]
----
cluster02::> cluster peer create -generate-passphrase -offer-expiration 2days

                     Passphrase: UCa+6lRVICXeL/gq1WrK7ShR
                Expiration Time: 6/7/2017 08:16:10 EST
  Initial Allowed Vserver Peers: -
            Intercluster LIF IP: 192.140.112.101
              Peer Cluster Name: Clus_7ShR (temporary generated)

Warning: make a note of the passphrase - it cannot be displayed again.
----
. 在源集群上，将源集群身份验证到目标集群：
+
`cluster peer create -peer-addrs _peer_LIF_IPs_ -ipspace _ipspace_s`

+
有关完整的命令语法，请参见手册页。

+
以下示例将本地集群通过集群间 LIF IP 地址 "192.140.112.101" 和 "192.140.112.102" 的远程集群进行身份验证：

+
[listing]
----
cluster01::> cluster peer create -peer-addrs 192.140.112.101,192.140.112.102

Notice: Use a generated passphrase or choose a passphrase of 8 or more characters.
        To ensure the authenticity of the peering relationship, use a phrase or sequence of characters that would be hard to guess.

Enter the passphrase:
Confirm the passphrase:

Clusters cluster02 and cluster01 are peered.
----
+
出现提示时，输入对等关系的密码短语。

. 验证是否已创建集群对等关系：
+
`cluster peer show -instance`

+
[listing]
----
cluster01::> cluster peer show -instance

                               Peer Cluster Name: cluster02
                   Remote Intercluster Addresses: 192.140.112.101, 192.140.112.102
              Availability of the Remote Cluster: Available
                             Remote Cluster Name: cluster2
                             Active IP Addresses: 192.140.112.101, 192.140.112.102
                           Cluster Serial Number: 1-80-123456
                  Address Family of Relationship: ipv4
            Authentication Status Administrative: no-authentication
               Authentication Status Operational: absent
                                Last Update Time: 02/05 21:05:41
                    IPspace for the Relationship: Default
----
. 检查对等关系中节点的连接和状态：
+
`集群对等运行状况显示`

+
[listing]
----
cluster01::> cluster peer health show
Node       cluster-Name                Node-Name
             Ping-Status               RDB-Health Cluster-Health  Avail…
---------- --------------------------- ---------  --------------- --------
cluster01-01
           cluster02                   cluster02-01
             Data: interface_reachable
             ICMP: interface_reachable true       true            true
                                       cluster02-02
             Data: interface_reachable
             ICMP: interface_reachable true       true            true
cluster01-02
           cluster02                   cluster02-01
             Data: interface_reachable
             ICMP: interface_reachable true       true            true
                                       cluster02-02
             Data: interface_reachable
             ICMP: interface_reachable true       true            true
----




=== 正在创建 DR 组

您必须在集群之间创建灾难恢复（ DR ）组关系。

您可以在 MetroCluster 配置中的一个集群上执行此操作步骤，以便在两个集群中的节点之间创建 DR 关系。


NOTE: 创建灾难恢复组后，无法更改灾难恢复关系。

image::../media/mcc_dr_groups_4_node.gif[MCC DR 组 4 节点]

.步骤
. 在每个节点上输入以下命令，以验证节点是否已准备好创建 DR 组：
+
MetroCluster configuration-settings show-status`

+
命令输出应显示节点已准备就绪：

+
[listing]
----
cluster_A::> metrocluster configuration-settings show-status
Cluster                    Node          Configuration Settings Status
-------------------------- ------------- --------------------------------
cluster_A                  node_A_1      ready for DR group create
                           node_A_2      ready for DR group create
2 entries were displayed.
----
+
[listing]
----
cluster_B::> metrocluster configuration-settings show-status
Cluster                    Node          Configuration Settings Status
-------------------------- ------------- --------------------------------
cluster_B                  node_B_1      ready for DR group create
                           node_B_2      ready for DR group create
2 entries were displayed.
----
. 创建 DR 组：
+
MetroCluster configuration-settings dr-group create -partner-cluster _partner-cluster-name_ -local-node _local-node-name_ -remote-node _remote-node-name_`

+
此命令仅发出一次。无需在配对集群上重复此操作。在命令中，您可以指定远程集群的名称以及配对集群上一个本地节点和一个节点的名称。

+
您指定的两个节点将配置为 DR 配对节点，而其他两个节点（未在命令中指定）将配置为 DR 组中的第二个 DR 对。输入此命令后，这些关系将无法更改。

+
以下命令将创建这些 DR 对：

+
** node_A_1 和 node_B_1
** node_A_2 和 node_B_2


+
[listing]
----
Cluster_A::> metrocluster configuration-settings dr-group create -partner-cluster cluster_B -local-node node_A_1 -remote-node node_B_1
[Job 27] Job succeeded: DR Group Create is successful.
----




=== 配置和连接 MetroCluster IP 接口

您必须配置用于复制每个节点的存储和非易失性缓存的 MetroCluster IP 接口。然后，使用 MetroCluster IP 接口建立连接。这将创建用于存储复制的 iSCSI 连接。

.关于此任务
--

NOTE: 您必须谨慎选择 MetroCluster IP 地址，因为在初始配置后无法更改它们。

--
* 您必须为每个节点创建两个接口。这些接口必须与 MetroCluster RCF 文件中定义的 VLAN 相关联。
* 您必须在同一 VLAN 中创建所有 MetroCluster IP 接口 "`A` " 端口，并在另一个 VLAN 中创建所有 MetroCluster IP 接口 "`B` " 端口。请参见 link:concept_considerations_mcip.html["MetroCluster IP 配置的注意事项"]。
+
--
[NOTE]
====
** 某些平台使用 VLAN 作为 MetroCluster IP 接口。默认情况下，这两个端口中的每个端口都使用不同的 VLAN ： 10 和 20 。您也可以在 MetroCluster configuration-settings interface create` 命令中使用 ` -vlan-id 参数` 指定一个大于 100 （ 101 到 4095 之间）的其他（非默认） VLAN 。
** 从 ONTAP 9.1.1 开始，如果您使用的是第 3 层配置，则在创建 MetroCluster IP 接口时还必须指定 ` 网关` 参数。请参见 link:../install-ip/concept_considerations_layer_3.html["第 3 层广域网的注意事项"]。


====
--
+
如果使用的MetroCluster 为10/20或大于100、则可以将以下平台型号添加到现有VLAN配置中。如果使用了任何其他VLAN、则无法将这些平台添加到现有配置中、因为无法配置MetroCluster 接口。如果您使用的是任何其他平台、则VLAN配置不相关、因为ONTAP 中不需要此配置。

+
|===


| AFF 平台 | FAS 平台 


 a| 
** AFF A220
** AFF A250
** AFF A400

 a| 
** FAS2750
** FAS500f
** FAS8300
** FAS8700


|===


示例中使用了以下 IP 地址和子网：

|===


| 节点 | 接口 | IP 地址 | 子网 


 a| 
node_A_1
 a| 
MetroCluster IP 接口 1
 a| 
10.1.1.1
 a| 
10.1.1/24



 a| 
MetroCluster IP 接口 2.
 a| 
10.1.2.1
 a| 
10.1.2/24



 a| 
node_A_2
 a| 
MetroCluster IP 接口 1
 a| 
10.1.1.2
 a| 
10.1.1/24



 a| 
MetroCluster IP 接口 2.
 a| 
10.1.2.2.
 a| 
10.1.2/24



 a| 
node_B_1
 a| 
MetroCluster IP 接口 1
 a| 
10.1.1.3.
 a| 
10.1.1/24



 a| 
MetroCluster IP 接口 2.
 a| 
10.1.2.3
 a| 
10.1.2/24



 a| 
node_B_2
 a| 
MetroCluster IP 接口 1
 a| 
10.1.1.4
 a| 
10.1.1/24



 a| 
MetroCluster IP 接口 2.
 a| 
10.1.2.4
 a| 
10.1.2/24

|===
MetroCluster IP 接口使用的物理端口取决于平台型号，如下表所示。

|===
| 平台型号 | MetroCluster IP 端口 | 注意 


 a| 
AFF A900和FAS9500
 a| 
e5b
 a| 



 a| 
e7b



 a| 
AFF A800
 a| 
e0b
 a| 



 a| 
e1b



 a| 
AFF A700 和 FAS900
 a| 
e5a
 a| 



 a| 
e5b



 a| 
AFF A400
 a| 
E3a
 a| 



 a| 
E3B



 a| 
AFF A320
 a| 
e0g
 a| 



 a| 
e0h



 a| 
AFF A300 和 FAS8200
 a| 
e1a
 a| 



 a| 
e1b



 a| 
AFF A220 和 FAS2750
 a| 
e0a
 a| 
在这些系统上，这些物理端口也用作集群接口。



 a| 
e0b



 a| 
AFF A250 和 FAS500f
 a| 
e0c
 a| 



 a| 
e0d



 a| 
FAS8300 和 FAS8700
 a| 
e0c
 a| 



 a| 
e0d

|===
以下示例中的端口使用情况适用于 AFF A700 或 FAS9000 系统。

.步骤
. 确认每个节点均已启用磁盘自动分配：
+
`s存储磁盘选项 show`

+
磁盘自动分配将按磁盘架分配池 0 和池 1 磁盘。

+
自动分配列指示是否已启用磁盘自动分配。

+
[listing]
----

Node        BKg. FW. Upd.  Auto Copy   Auto Assign  Auto Assign Policy
----------  -------------  ----------  -----------  ------------------
node_A_1             on           on           on           default
node_A_2             on           on           on           default
2 entries were displayed.
----
. 验证是否可以在节点上创建 MetroCluster IP 接口：
+
MetroCluster configuration-settings show-status`

+
所有节点均应准备就绪：

+
[listing]
----

Cluster       Node         Configuration Settings Status
----------    -----------  ---------------------------------
cluster_A
              node_A_1     ready for interface create
              node_A_2     ready for interface create
cluster_B
              node_B_1     ready for interface create
              node_B_2     ready for interface create
4 entries were displayed.
----
. 在 "`node_A_1` " 上创建接口。
+
--
[NOTE]
====
** 以下示例中的端口使用情况适用于 AFF A700 或 FAS9000 系统（ e5a 和 e5b ）。您必须按照上述说明在适用于您的平台型号的正确端口上配置接口。
** 从 ONTAP 9.1.1 开始，如果您使用的是第 3 层配置，则在创建 MetroCluster IP 接口时还必须指定 ` 网关` 参数。请参见 link:concept_considerations_layer_3.html["第 3 层广域网的注意事项"]。
** 在支持 MetroCluster IP 接口的 的平台型号上，如果不想使用默认 VLAN ID ，可以使用 ` -vlan-id` 参数。


====
--
+
.. 在端口 "`node_A_1` " 上配置接口 "`e5a` " ：
+
MetroCluster configuration-settings interface create -cluster-name _cluster-name_ -home-node _node-name_ -home-port e5a -address _ip-address_ -netmask _netmask_`

+
以下示例显示了如何在 "node_A_1" 上的端口 "e5a" 上创建 IP 地址为 10.1.1.1" 的接口：

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface create -cluster-name cluster_A -home-node node_A_1 -home-port e5a -address 10.1.1.1 -netmask 255.255.255.0
[Job 28] Job succeeded: Interface Create is successful.
cluster_A::>
----
.. 在端口 "`node_A_1` " 上配置接口 "`e5b` " ：
+
MetroCluster configuration-settings interface create -cluster-name _cluster-name_ -home-node _node-name_ -home-port e5b -address _ip-address_ -netmask _netmask_`

+
以下示例显示了在 IP 地址为 "`10.1.2.1` " 的 "`node_A_1` " 上的端口 "`e5b` " 上创建的接口：

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface create -cluster-name cluster_A -home-node node_A_1 -home-port e5b -address 10.1.2.1 -netmask 255.255.255.0
[Job 28] Job succeeded: Interface Create is successful.
cluster_A::>
----


+

NOTE: 您可以使用 `MetroCluster configuration-settings interface show` 命令验证这些接口是否存在。

. 在 "`node_A_2` " 上创建接口。
+
--
[NOTE]
====
** 以下示例中的端口使用情况适用于 AFF A700 或 FAS9000 系统（ "`e5a` " 和 "`e5b` " ）。您必须按照上述说明在适用于您的平台型号的正确端口上配置接口。
** 从 ONTAP 9.1.1 开始，如果您使用的是第 3 层配置，则在创建 MetroCluster IP 接口时还必须指定 ` 网关` 参数。请参见 link:concept_considerations_layer_3.html["第 3 层广域网的注意事项"]。
** 在支持 MetroCluster IP 接口的 的平台型号上，如果不想使用默认 VLAN ID ，可以使用 ` -vlan-id` 参数。


====
--
+
.. 在端口 "`node_A_2` " 上配置接口 "`e5a` " ：
+
MetroCluster configuration-settings interface create -cluster-name _cluster-name_ -home-node _node-name_ -home-port e5a -address _ip-address_ -netmask _netmask_`

+
以下示例显示了在 IP 地址为 "`10.1.1.2` " 的 "`node_A_2` " 上的端口 "`e5a` " 上创建的接口：

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface create -cluster-name cluster_A -home-node node_A_2 -home-port e5a -address 10.1.1.2 -netmask 255.255.255.0
[Job 28] Job succeeded: Interface Create is successful.
cluster_A::>
----
+
在支持 MetroCluster IP 接口的 VLAN 的平台型号上，如果不想使用默认 VLAN ID ，可以使用 ` -vlan-id` 参数。以下示例显示了 VLAN ID 为 "`120` " 的 AFF A220 系统的命令：

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface create -cluster-name cluster_A -home-node node_A_2 -home-port e0a -address 10.1.1.2 -netmask 255.255.255.0 -vlan-id 120
[Job 28] Job succeeded: Interface Create is successful.
cluster_A::>
----
.. 在端口 "`node_A_2` " 上配置接口 "`e5b` " ：
+
MetroCluster configuration-settings interface create -cluster-name _cluster-name_ -home-node _node-name_ -home-port e5b -address _ip-address_ -netmask _netmask_`

+
以下示例显示了在 IP 地址为 "`10.1.2.2` " 的 "`node_A_2` " 上的端口 "`e5b` " 上创建的接口：

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface create -cluster-name cluster_A -home-node node_A_2 -home-port e5b -address 10.1.2.2 -netmask 255.255.255.0
[Job 28] Job succeeded: Interface Create is successful.
cluster_A::>
----
+
在支持 MetroCluster IP 接口的 VLAN 的平台型号上，如果不想使用默认 VLAN ID ，可以使用 ` -vlan-id` 参数。以下示例显示了 VLAN ID 为 "`220` " 的 AFF A220 系统的命令：

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface create -cluster-name cluster_A -home-node node_A_2 -home-port e0b -address 10.1.2.2 -netmask 255.255.255.0 -vlan-id 220
[Job 28] Job succeeded: Interface Create is successful.
cluster_A::>
----


. 在 "`node_B_1` " 上创建接口。
+
--
[NOTE]
====
** 以下示例中的端口使用情况适用于 AFF A700 或 FAS9000 系统（ "`e5a` " 和 "`e5b` " ）。您必须按照上述说明在适用于您的平台型号的正确端口上配置接口。
** 从 ONTAP 9.1.1 开始，如果您使用的是第 3 层配置，则在创建 MetroCluster IP 接口时还必须指定 ` 网关` 参数。请参见 link:concept_considerations_layer_3.html["第 3 层广域网的注意事项"]。
** 在支持 MetroCluster IP 接口的 的平台型号上，如果不想使用默认 VLAN ID ，可以使用 ` -vlan-id` 参数。


====
--
+
.. 在端口 "`node_B_1` " 上配置接口 "`e5a` " ：
+
MetroCluster configuration-settings interface create -cluster-name _cluster-name_ -home-node _node-name_ -home-port e5a -address _ip-address_ -netmask _netmask_`

+
以下示例显示了在 IP 地址为 "`10.1.1.3` " 的 "`node_B_1` " 上的端口 "`e5a` " 上创建的接口：

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface create -cluster-name cluster_A -home-node node_B_1 -home-port e5a -address 10.1.1.3 -netmask 255.255.255.0
[Job 28] Job succeeded: Interface Create is successful.cluster_A::>
----
.. 在端口 "`node_B_1` " 上配置接口 "`e5b` " ：
+
MetroCluster configuration-settings interface create -cluster-name _cluster-name_ -home-node _node-name_ -home-port e5a -address _ip-address_ -netmask _netmask_`

+
以下示例显示了在 IP 地址为 "`10.1.2.3` " 的 "`node_B_1` " 上的端口 "e5b" 上创建的接口：

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface create -cluster-name cluster_A -home-node node_B_1 -home-port e5b -address 10.1.2.3 -netmask 255.255.255.0
[Job 28] Job succeeded: Interface Create is successful.cluster_A::>
----


. 在 "`node_B_2` " 上创建接口。
+
--
[NOTE]
====
** 以下示例中的端口使用情况适用于 AFF A700 或 FAS9000 系统（ e5a 和 e5b ）。您必须按照上述说明在适用于您的平台型号的正确端口上配置接口。
** 从 ONTAP 9.1.1 开始，如果您使用的是第 3 层配置，则在创建 MetroCluster IP 接口时还必须指定 ` 网关` 参数。请参见 link:concept_considerations_layer_3.html["第 3 层广域网的注意事项"]。
** 在支持 MetroCluster IP 接口的 的平台型号上，如果不想使用默认 VLAN ID ，可以使用 ` -vlan-id` 参数。


====
--
+
.. 在端口 "`node_B_2` " 上配置接口 "`e5a` " ：
+
MetroCluster configuration-settings interface create -cluster-name _cluster-name_ -home-node _node-name_ -home-port e5a -address _ip-address_ -netmask _netmask_`

+
以下示例显示了在 IP 地址为 "`10.1.1.4` " 的 "`node_B_2` " 上的端口 "`e5a` " 上创建的接口：

+
[listing]
----
cluster_B::>metrocluster configuration-settings interface create -cluster-name cluster_B -home-node node_B_2 -home-port e5a -address 10.1.1.4 -netmask 255.255.255.0
[Job 28] Job succeeded: Interface Create is successful.cluster_A::>
----
.. 在端口 "`node_B_2` " 上配置接口 "`e5b` " ：
+
MetroCluster configuration-settings interface create -cluster-name _cluster-name_ -home-node _node-name_ -home-port e5b -address _ip-address_ -netmask _netmask_`

+
以下示例显示了在 IP 地址为 "`10.1.2.4` " 的 "`node_B_2` " 上的端口 "`e5b` " 上创建的接口：

+
[listing]
----
cluster_B::> metrocluster configuration-settings interface create -cluster-name cluster_B -home-node node_B_2 -home-port e5b -address 10.1.2.4 -netmask 255.255.255.0
[Job 28] Job succeeded: Interface Create is successful.
cluster_A::>
----


. 验证是否已配置接口：
+
`MetroCluster configuration-settings interface show`

+
以下示例显示了每个接口的配置状态已完成。

+
[listing]
----
cluster_A::> metrocluster configuration-settings interface show
DR                                                              Config
Group Cluster Node    Network Address Netmask         Gateway   State
----- ------- ------- --------------- --------------- --------- ----------
1     cluster_A  node_A_1
                 Home Port: e5a
                      10.1.1.1     255.255.255.0   -         completed
                 Home Port: e5b
                      10.1.2.1     255.255.255.0   -         completed
                 node_A_2
                 Home Port: e5a
                      10.1.1.2     255.255.255.0   -         completed
                 Home Port: e5b
                      10.1.2.2     255.255.255.0   -         completed
      cluster_B  node_B_1
                 Home Port: e5a
                      10.1.1.3     255.255.255.0   -         completed
                 Home Port: e5b
                      10.1.2.3     255.255.255.0   -         completed
                 node_B_2
                 Home Port: e5a
                      10.1.1.4     255.255.255.0   -         completed
                 Home Port: e5b
                      10.1.2.4     255.255.255.0   -         completed
8 entries were displayed.
cluster_A::>
----
. 验证节点是否已准备好连接 MetroCluster 接口：
+
MetroCluster configuration-settings show-status`

+
以下示例显示了处于 " `re连接就绪` " 状态的所有节点：

+
[listing]
----

Cluster       Node         Configuration Settings Status
----------    -----------  ---------------------------------
cluster_A
              node_A_1     ready for connection connect
              node_A_2     ready for connection connect
cluster_B
              node_B_1     ready for connection connect
              node_B_2     ready for connection connect
4 entries were displayed.
----
. 建立连接：
+
`MetroCluster configuration-settings connection connect`

+
问题描述此命令后，无法更改 IP 地址。

+
以下示例显示 "`cluster_A` " 已成功连接：

+
[listing]
----
cluster_A::> metrocluster configuration-settings connection connect
[Job 53] Job succeeded: Connect is successful.
cluster_A::>
----
. 验证是否已建立连接：
+
MetroCluster configuration-settings show-status`

+
所有节点的配置设置状态均应为已完成：

+
[listing]
----

Cluster       Node         Configuration Settings Status
----------    -----------  ---------------------------------
cluster_A
              node_A_1     completed
              node_A_2     completed
cluster_B
              node_B_1     completed
              node_B_2     completed
4 entries were displayed.
----
. 验证是否已建立 iSCSI 连接：
+
.. 更改为高级权限级别：
+
`set -privilege advanced`

+
当系统提示您继续进入高级模式且您看到高级模式提示符（` * >` ）时，您需要使用 "`y` " 进行响应。

.. 显示连接：
+
`storage iscsi-initiator show`

+
在运行 ONTAP 9.5 的系统上，每个集群上应有八个 MetroCluster IP 启动程序，这些启动程序应显示在输出中。

+
在运行 ONTAP 9.4 及更早版本的系统上，每个集群上应有四个 MetroCluster IP 启动程序，这些启动程序应显示在输出中。

+
以下示例显示了运行 ONTAP 9.5 的集群上的八个 MetroCluster IP 启动程序：

+
[listing]
----
cluster_A::*> storage iscsi-initiator show
Node Type Label    Target Portal           Target Name                      Admin/Op
---- ---- -------- ------------------      -------------------------------- --------

cluster_A-01
     dr_auxiliary
              mccip-aux-a-initiator
                   10.227.16.113:65200     prod506.com.company:abab44       up/up
              mccip-aux-a-initiator2
                   10.227.16.113:65200     prod507.com.company:abab44       up/up
              mccip-aux-b-initiator
                   10.227.95.166:65200     prod506.com.company:abab44       up/up
              mccip-aux-b-initiator2
                   10.227.95.166:65200     prod507.com.company:abab44       up/up
     dr_partner
              mccip-pri-a-initiator
                   10.227.16.112:65200     prod506.com.company:cdcd88       up/up
              mccip-pri-a-initiator2
                   10.227.16.112:65200     prod507.com.company:cdcd88       up/up
              mccip-pri-b-initiator
                   10.227.95.165:65200     prod506.com.company:cdcd88       up/up
              mccip-pri-b-initiator2
                   10.227.95.165:65200     prod507.com.company:cdcd88       up/up
cluster_A-02
     dr_auxiliary
              mccip-aux-a-initiator
                   10.227.16.112:65200     prod506.com.company:cdcd88       up/up
              mccip-aux-a-initiator2
                   10.227.16.112:65200     prod507.com.company:cdcd88       up/up
              mccip-aux-b-initiator
                   10.227.95.165:65200     prod506.com.company:cdcd88       up/up
              mccip-aux-b-initiator2
                   10.227.95.165:65200     prod507.com.company:cdcd88       up/up
     dr_partner
              mccip-pri-a-initiator
                   10.227.16.113:65200     prod506.com.company:abab44       up/up
              mccip-pri-a-initiator2
                   10.227.16.113:65200     prod507.com.company:abab44       up/up
              mccip-pri-b-initiator
                   10.227.95.166:65200     prod506.com.company:abab44       up/up
              mccip-pri-b-initiator2
                   10.227.95.166:65200     prod507.com.company:abab44       up/up
16 entries were displayed.
----
.. 返回到管理权限级别：
+
`set -privilege admin`



. 验证节点是否已准备好最终实施 MetroCluster 配置：
+
`MetroCluster node show`

+
[listing]
----
cluster_A::> metrocluster node show
DR                               Configuration  DR
Group Cluster Node               State          Mirroring Mode
----- ------- ------------------ -------------- --------- ----
-     cluster_A
              node_A_1           ready to configure -     -
              node_A_2           ready to configure -     -
2 entries were displayed.
cluster_A::>
----
+
[listing]
----
cluster_B::> metrocluster node show
DR                               Configuration  DR
Group Cluster Node               State          Mirroring Mode
----- ------- ------------------ -------------- --------- ----
-     cluster_B
              node_B_1           ready to configure -     -
              node_B_2           ready to configure -     -
2 entries were displayed.
cluster_B::>
----




=== 验证或手动执行池 1 驱动器分配

根据存储配置的不同，您必须验证池 1 驱动器分配情况，或者为 MetroCluster IP 配置中的每个节点手动将驱动器分配到池 1 。

您使用的操作步骤取决于所使用的 ONTAP 版本。

|===


| 配置类型 | 操作步骤 


 a| 
这些系统满足驱动器自动分配的要求，或者，如果运行 ONTAP 9.3 ，则是从工厂收到的。
 a| 
<<Verifying disk assignment for pool 1 disks>>



 a| 
此配置包括三个磁盘架，或者如果其包含四个以上的磁盘架，则包含四个磁盘架中不均匀的多个（例如七个磁盘架），并且正在运行 ONTAP 9.5 。
 a| 
<<Manually assigning drives for pool 1 (ONTAP 9.4 or later)>>



 a| 
此配置不包括每个站点四个存储架，并且运行的是 ONTAP 9.4
 a| 
<<Manually assigning drives for pool 1 (ONTAP 9.4 or later)>>



 a| 
系统未从工厂收到，并且运行的是 ONTAP 9.3 从工厂收到的系统已预先配置分配的驱动器。
 a| 
<<Manually assigning disks for pool 1 (ONTAP 9.3)>>

|===


==== 验证池 1 磁盘的磁盘分配

您必须验证远程磁盘对节点可见且已正确分配。

使用 `MetroCluster configuration-settings connection connect` 命令创建 MetroCluster IP 接口和连接后，必须至少等待十分钟才能完成磁盘自动分配。

命令输出将以以下格式显示磁盘名称：

`node-name ： 0m.i1.0L1`

link:concept_considerations_drive_assignment.html["ONTAP 9.4 及更高版本中的自动驱动器分配和 ADP 系统注意事项"]

.步骤
. 验证池 1 磁盘是否已自动分配：
+
`d展示`

+
以下输出显示了没有外部磁盘架的 AFF A800 系统的输出。

+
驱动器自动分配已将四分之一（ 8 个驱动器）分配给 "`node_A_1` " ，将四分之一分配给 "`node_A_2` " 。其余驱动器将是 "`node_B_1` " 和 "`node_B_2` " 的远程（ pool1 ）磁盘。

+
[listing]
----
cluster_B::> disk show -host-adapter 0m -owner node_B_2
                    Usable     Disk              Container   Container
Disk                Size       Shelf Bay Type    Type        Name      Owner
----------------    ---------- ----- --- ------- ----------- --------- --------
node_B_2:0m.i0.2L4  894.0GB    0     29  SSD-NVM shared      -         node_B_2
node_B_2:0m.i0.2L10 894.0GB    0     25  SSD-NVM shared      -         node_B_2
node_B_2:0m.i0.3L3  894.0GB    0     28  SSD-NVM shared      -         node_B_2
node_B_2:0m.i0.3L9  894.0GB    0     24  SSD-NVM shared      -         node_B_2
node_B_2:0m.i0.3L11 894.0GB    0     26  SSD-NVM shared      -         node_B_2
node_B_2:0m.i0.3L12 894.0GB    0     27  SSD-NVM shared      -         node_B_2
node_B_2:0m.i0.3L15 894.0GB    0     30  SSD-NVM shared      -         node_B_2
node_B_2:0m.i0.3L16 894.0GB    0     31  SSD-NVM shared      -         node_B_2
8 entries were displayed.

cluster_B::> disk show -host-adapter 0m -owner node_B_1
                    Usable     Disk              Container   Container
Disk                Size       Shelf Bay Type    Type        Name      Owner
----------------    ---------- ----- --- ------- ----------- --------- --------
node_B_1:0m.i2.3L19 1.75TB     0     42  SSD-NVM shared      -         node_B_1
node_B_1:0m.i2.3L20 1.75TB     0     43  SSD-NVM spare       Pool1     node_B_1
node_B_1:0m.i2.3L23 1.75TB     0     40  SSD-NVM shared       -        node_B_1
node_B_1:0m.i2.3L24 1.75TB     0     41  SSD-NVM spare       Pool1     node_B_1
node_B_1:0m.i2.3L29 1.75TB     0     36  SSD-NVM shared       -        node_B_1
node_B_1:0m.i2.3L30 1.75TB     0     37  SSD-NVM shared       -        node_B_1
node_B_1:0m.i2.3L31 1.75TB     0     38  SSD-NVM shared       -        node_B_1
node_B_1:0m.i2.3L32 1.75TB     0     39  SSD-NVM shared       -        node_B_1
8 entries were displayed.

cluster_B::> disk show
                    Usable     Disk              Container   Container
Disk                Size       Shelf Bay Type    Type        Name      Owner
----------------    ---------- ----- --- ------- ----------- --------- --------
node_B_1:0m.i1.0L6  1.75TB     0     1   SSD-NVM shared      -         node_A_2
node_B_1:0m.i1.0L8  1.75TB     0     3   SSD-NVM shared      -         node_A_2
node_B_1:0m.i1.0L17 1.75TB     0     18  SSD-NVM shared      -         node_A_1
node_B_1:0m.i1.0L22 1.75TB     0     17 SSD-NVM shared - node_A_1
node_B_1:0m.i1.0L25 1.75TB     0     12 SSD-NVM shared - node_A_1
node_B_1:0m.i1.2L2  1.75TB     0     5 SSD-NVM shared - node_A_2
node_B_1:0m.i1.2L7  1.75TB     0     2 SSD-NVM shared - node_A_2
node_B_1:0m.i1.2L14 1.75TB     0     7 SSD-NVM shared - node_A_2
node_B_1:0m.i1.2L21 1.75TB     0     16 SSD-NVM shared - node_A_1
node_B_1:0m.i1.2L27 1.75TB     0     14 SSD-NVM shared - node_A_1
node_B_1:0m.i1.2L28 1.75TB     0     15 SSD-NVM shared - node_A_1
node_B_1:0m.i2.1L1  1.75TB     0     4 SSD-NVM shared - node_A_2
node_B_1:0m.i2.1L5  1.75TB     0     0 SSD-NVM shared - node_A_2
node_B_1:0m.i2.1L13 1.75TB     0     6 SSD-NVM shared - node_A_2
node_B_1:0m.i2.1L18 1.75TB     0     19 SSD-NVM shared - node_A_1
node_B_1:0m.i2.1L26 1.75TB     0     13 SSD-NVM shared - node_A_1
node_B_1:0m.i2.3L19 1.75TB     0 42 SSD-NVM shared - node_B_1
node_B_1:0m.i2.3L20 1.75TB     0 43 SSD-NVM shared - node_B_1
node_B_1:0m.i2.3L23 1.75TB     0 40 SSD-NVM shared - node_B_1
node_B_1:0m.i2.3L24 1.75TB     0 41 SSD-NVM shared - node_B_1
node_B_1:0m.i2.3L29 1.75TB     0 36 SSD-NVM shared - node_B_1
node_B_1:0m.i2.3L30 1.75TB     0 37 SSD-NVM shared - node_B_1
node_B_1:0m.i2.3L31 1.75TB     0 38 SSD-NVM shared - node_B_1
node_B_1:0m.i2.3L32 1.75TB     0 39 SSD-NVM shared - node_B_1
node_B_1:0n.12      1.75TB     0 12 SSD-NVM shared aggr0 node_B_1
node_B_1:0n.13      1.75TB     0 13 SSD-NVM shared aggr0 node_B_1
node_B_1:0n.14      1.75TB     0 14 SSD-NVM shared aggr0 node_B_1
node_B_1:0n.15      1.75TB 0 15 SSD-NVM shared aggr0 node_B_1
node_B_1:0n.16      1.75TB 0 16 SSD-NVM shared aggr0 node_B_1
node_B_1:0n.17      1.75TB 0 17 SSD-NVM shared aggr0 node_B_1
node_B_1:0n.18      1.75TB 0 18 SSD-NVM shared aggr0 node_B_1
node_B_1:0n.19      1.75TB 0 19 SSD-NVM shared - node_B_1
node_B_1:0n.24      894.0GB 0 24 SSD-NVM shared - node_A_2
node_B_1:0n.25      894.0GB 0 25 SSD-NVM shared - node_A_2
node_B_1:0n.26      894.0GB 0 26 SSD-NVM shared - node_A_2
node_B_1:0n.27      894.0GB 0 27 SSD-NVM shared - node_A_2
node_B_1:0n.28      894.0GB 0 28 SSD-NVM shared - node_A_2
node_B_1:0n.29      894.0GB 0 29 SSD-NVM shared - node_A_2
node_B_1:0n.30      894.0GB 0 30 SSD-NVM shared - node_A_2
node_B_1:0n.31      894.0GB 0 31 SSD-NVM shared - node_A_2
node_B_1:0n.36      1.75TB 0 36 SSD-NVM shared - node_A_1
node_B_1:0n.37      1.75TB 0 37 SSD-NVM shared - node_A_1
node_B_1:0n.38      1.75TB 0 38 SSD-NVM shared - node_A_1
node_B_1:0n.39      1.75TB 0 39 SSD-NVM shared - node_A_1
node_B_1:0n.40      1.75TB 0 40 SSD-NVM shared - node_A_1
node_B_1:0n.41      1.75TB 0 41 SSD-NVM shared - node_A_1
node_B_1:0n.42      1.75TB 0 42 SSD-NVM shared - node_A_1
node_B_1:0n.43      1.75TB 0 43 SSD-NVM shared - node_A_1
node_B_2:0m.i0.2L4  894.0GB 0 29 SSD-NVM shared - node_B_2
node_B_2:0m.i0.2L10 894.0GB 0 25 SSD-NVM shared - node_B_2
node_B_2:0m.i0.3L3  894.0GB 0 28 SSD-NVM shared - node_B_2
node_B_2:0m.i0.3L9  894.0GB 0 24 SSD-NVM shared - node_B_2
node_B_2:0m.i0.3L11 894.0GB 0 26 SSD-NVM shared - node_B_2
node_B_2:0m.i0.3L12 894.0GB 0 27 SSD-NVM shared - node_B_2
node_B_2:0m.i0.3L15 894.0GB 0 30 SSD-NVM shared - node_B_2
node_B_2:0m.i0.3L16 894.0GB 0 31 SSD-NVM shared - node_B_2
node_B_2:0n.0       1.75TB 0 0 SSD-NVM shared aggr0_rha12_b1_cm_02_0 node_B_2
node_B_2:0n.1 1.75TB 0 1 SSD-NVM shared aggr0_rha12_b1_cm_02_0 node_B_2
node_B_2:0n.2 1.75TB 0 2 SSD-NVM shared aggr0_rha12_b1_cm_02_0 node_B_2
node_B_2:0n.3 1.75TB 0 3 SSD-NVM shared aggr0_rha12_b1_cm_02_0 node_B_2
node_B_2:0n.4 1.75TB 0 4 SSD-NVM shared aggr0_rha12_b1_cm_02_0 node_B_2
node_B_2:0n.5 1.75TB 0 5 SSD-NVM shared aggr0_rha12_b1_cm_02_0 node_B_2
node_B_2:0n.6 1.75TB 0 6 SSD-NVM shared aggr0_rha12_b1_cm_02_0 node_B_2
node_B_2:0n.7 1.75TB 0 7 SSD-NVM shared - node_B_2
64 entries were displayed.

cluster_B::>


cluster_A::> disk show
Usable Disk Container Container
Disk Size Shelf Bay Type Type Name Owner
---------------- ---------- ----- --- ------- ----------- --------- --------
node_A_1:0m.i1.0L2 1.75TB 0 5 SSD-NVM shared - node_B_2
node_A_1:0m.i1.0L8 1.75TB 0 3 SSD-NVM shared - node_B_2
node_A_1:0m.i1.0L18 1.75TB 0 19 SSD-NVM shared - node_B_1
node_A_1:0m.i1.0L25 1.75TB 0 12 SSD-NVM shared - node_B_1
node_A_1:0m.i1.0L27 1.75TB 0 14 SSD-NVM shared - node_B_1
node_A_1:0m.i1.2L1 1.75TB 0 4 SSD-NVM shared - node_B_2
node_A_1:0m.i1.2L6 1.75TB 0 1 SSD-NVM shared - node_B_2
node_A_1:0m.i1.2L7 1.75TB 0 2 SSD-NVM shared - node_B_2
node_A_1:0m.i1.2L14 1.75TB 0 7 SSD-NVM shared - node_B_2
node_A_1:0m.i1.2L17 1.75TB 0 18 SSD-NVM shared - node_B_1
node_A_1:0m.i1.2L22 1.75TB 0 17 SSD-NVM shared - node_B_1
node_A_1:0m.i2.1L5 1.75TB 0 0 SSD-NVM shared - node_B_2
node_A_1:0m.i2.1L13 1.75TB 0 6 SSD-NVM shared - node_B_2
node_A_1:0m.i2.1L21 1.75TB 0 16 SSD-NVM shared - node_B_1
node_A_1:0m.i2.1L26 1.75TB 0 13 SSD-NVM shared - node_B_1
node_A_1:0m.i2.1L28 1.75TB 0 15 SSD-NVM shared - node_B_1
node_A_1:0m.i2.3L19 1.75TB 0 42 SSD-NVM shared - node_A_1
node_A_1:0m.i2.3L20 1.75TB 0 43 SSD-NVM shared - node_A_1
node_A_1:0m.i2.3L23 1.75TB 0 40 SSD-NVM shared - node_A_1
node_A_1:0m.i2.3L24 1.75TB 0 41 SSD-NVM shared - node_A_1
node_A_1:0m.i2.3L29 1.75TB 0 36 SSD-NVM shared - node_A_1
node_A_1:0m.i2.3L30 1.75TB 0 37 SSD-NVM shared - node_A_1
node_A_1:0m.i2.3L31 1.75TB 0 38 SSD-NVM shared - node_A_1
node_A_1:0m.i2.3L32 1.75TB 0 39 SSD-NVM shared - node_A_1
node_A_1:0n.12 1.75TB 0 12 SSD-NVM shared aggr0 node_A_1
node_A_1:0n.13 1.75TB 0 13 SSD-NVM shared aggr0 node_A_1
node_A_1:0n.14 1.75TB 0 14 SSD-NVM shared aggr0 node_A_1
node_A_1:0n.15 1.75TB 0 15 SSD-NVM shared aggr0 node_A_1
node_A_1:0n.16 1.75TB 0 16 SSD-NVM shared aggr0 node_A_1
node_A_1:0n.17 1.75TB 0 17 SSD-NVM shared aggr0 node_A_1
node_A_1:0n.18 1.75TB 0 18 SSD-NVM shared aggr0 node_A_1
node_A_1:0n.19 1.75TB 0 19 SSD-NVM shared - node_A_1
node_A_1:0n.24 894.0GB 0 24 SSD-NVM shared - node_B_2
node_A_1:0n.25 894.0GB 0 25 SSD-NVM shared - node_B_2
node_A_1:0n.26 894.0GB 0 26 SSD-NVM shared - node_B_2
node_A_1:0n.27 894.0GB 0 27 SSD-NVM shared - node_B_2
node_A_1:0n.28 894.0GB 0 28 SSD-NVM shared - node_B_2
node_A_1:0n.29 894.0GB 0 29 SSD-NVM shared - node_B_2
node_A_1:0n.30 894.0GB 0 30 SSD-NVM shared - node_B_2
node_A_1:0n.31 894.0GB 0 31 SSD-NVM shared - node_B_2
node_A_1:0n.36 1.75TB 0 36 SSD-NVM shared - node_B_1
node_A_1:0n.37 1.75TB 0 37 SSD-NVM shared - node_B_1
node_A_1:0n.38 1.75TB 0 38 SSD-NVM shared - node_B_1
node_A_1:0n.39 1.75TB 0 39 SSD-NVM shared - node_B_1
node_A_1:0n.40 1.75TB 0 40 SSD-NVM shared - node_B_1
node_A_1:0n.41 1.75TB 0 41 SSD-NVM shared - node_B_1
node_A_1:0n.42 1.75TB 0 42 SSD-NVM shared - node_B_1
node_A_1:0n.43 1.75TB 0 43 SSD-NVM shared - node_B_1
node_A_2:0m.i2.3L3 894.0GB 0 28 SSD-NVM shared - node_A_2
node_A_2:0m.i2.3L4 894.0GB 0 29 SSD-NVM shared - node_A_2
node_A_2:0m.i2.3L9 894.0GB 0 24 SSD-NVM shared - node_A_2
node_A_2:0m.i2.3L10 894.0GB 0 25 SSD-NVM shared - node_A_2
node_A_2:0m.i2.3L11 894.0GB 0 26 SSD-NVM shared - node_A_2
node_A_2:0m.i2.3L12 894.0GB 0 27 SSD-NVM shared - node_A_2
node_A_2:0m.i2.3L15 894.0GB 0 30 SSD-NVM shared - node_A_2
node_A_2:0m.i2.3L16 894.0GB 0 31 SSD-NVM shared - node_A_2
node_A_2:0n.0 1.75TB 0 0 SSD-NVM shared aggr0_node_A_2_0 node_A_2
node_A_2:0n.1 1.75TB 0 1 SSD-NVM shared aggr0_node_A_2_0 node_A_2
node_A_2:0n.2 1.75TB 0 2 SSD-NVM shared aggr0_node_A_2_0 node_A_2
node_A_2:0n.3 1.75TB 0 3 SSD-NVM shared aggr0_node_A_2_0 node_A_2
node_A_2:0n.4 1.75TB 0 4 SSD-NVM shared aggr0_node_A_2_0 node_A_2
node_A_2:0n.5 1.75TB 0 5 SSD-NVM shared aggr0_node_A_2_0 node_A_2
node_A_2:0n.6 1.75TB 0 6 SSD-NVM shared aggr0_node_A_2_0 node_A_2
node_A_2:0n.7 1.75TB 0 7 SSD-NVM shared - node_A_2
64 entries were displayed.

cluster_A::>
----




==== 手动为池 1 分配驱动器（ ONTAP 9.4 或更高版本）

如果系统在出厂时未进行预配置，并且不满足自动驱动器分配的要求，则必须手动分配远程池 1 驱动器。

此操作步骤适用场景配置运行 ONTAP 9.4 或更高版本。

有关确定系统是否需要手动分配磁盘的详细信息，请参见 link:concept_considerations_drive_assignment.html["ONTAP 9.4 及更高版本中的自动驱动器分配和 ADP 系统注意事项"]。

如果配置中每个站点仅包含两个外部磁盘架，则每个站点的池 1 驱动器应从相同磁盘架中共享，如以下示例所示：

* 在 site_B-shelf_2 （远程）上的托架 0-11 中为 node_A_1 分配了驱动器
* 在 site_B-shelf_2 （远程）上的托架 12-23 中为 node_A_2 分配了驱动器


.步骤
. 在 MetroCluster IP 配置中的每个节点上，将远程驱动器分配给池 1 。
+
.. 显示未分配驱动器的列表：
+
`disk show -host-adapter 0m -container-type unassigned`

+
[listing]
----
cluster_A::> disk show -host-adapter 0m -container-type unassigned
                     Usable           Disk    Container   Container
Disk                   Size Shelf Bay Type    Type        Name      Owner
---------------- ---------- ----- --- ------- ----------- --------- --------
6.23.0                    -    23   0 SSD     unassigned  -         -
6.23.1                    -    23   1 SSD     unassigned  -         -
.
.
.
node_A_2:0m.i1.2L51       -    21  14 SSD     unassigned  -         -
node_A_2:0m.i1.2L64       -    21  10 SSD     unassigned  -         -
.
.
.
48 entries were displayed.

cluster_A::>
----
.. 将远程驱动器（ 0m ）的所有权分配给第一个节点的池 1 （例如， "`node_A_1` " ）：
+
`ddisk assign -disk _disk-id_ -pool 1 -owner _owner-node-name_`

+
`disk-id` 必须标识远程磁盘架 `owner-node-name` 上的驱动器。

.. 确认驱动器已分配给池 1 ：
+
`disk show -host-adapter 0m -container-type unassigned`

+
--

NOTE: 用于访问远程驱动器的 iSCSI 连接显示为设备 0m 。

--
+
以下输出显示已分配磁盘架 "`23` " 上的驱动器，因为这些驱动器不再出现在未分配的驱动器列表中：

+
[listing]
----
cluster_A::> disk show -host-adapter 0m -container-type unassigned
                     Usable           Disk    Container   Container
Disk                   Size Shelf Bay Type    Type        Name      Owner
---------------- ---------- ----- --- ------- ----------- --------- --------
node_A_2:0m.i1.2L51       -    21  14 SSD     unassigned  -         -
node_A_2:0m.i1.2L64       -    21  10 SSD     unassigned  -         -
.
.
.
node_A_2:0m.i2.1L90       -    21  19 SSD     unassigned  -         -
24 entries were displayed.

cluster_A::>
----
.. 重复上述步骤，将池 1 驱动器分配给站点 A 上的第二个节点（例如， "`node_A_2` " ）。
.. 在站点 B 上重复这些步骤






==== 手动为池 1 分配磁盘（ ONTAP 9.3 ）

如果每个节点至少有两个磁盘架，则可以使用 ONTAP 的自动分配功能自动分配远程（ pool1 ）磁盘。

您必须先将磁盘架上的磁盘分配给 pool1 。然后， ONTAP 会自动将磁盘架上的其余磁盘分配到同一个池。

此操作步骤适用场景配置运行 ONTAP 9.3 。

只有当每个节点至少有两个磁盘架时，才可以使用此操作步骤，从而可以在磁盘架级别自动分配磁盘。

如果不能使用磁盘架级别的自动分配，则必须手动分配远程磁盘，以便每个节点都有一个远程磁盘池（池 1 ）。

ONTAP 自动磁盘分配功能可按磁盘架分配磁盘。例如：

* site_B-shelf_2 上的所有磁盘都会自动分配给 node_A_1 的 pool1
* site_B-shelf_4 上的所有磁盘都会自动分配给 node_A_2 的 pool1
* site_A-shelf_2 上的所有磁盘都会自动分配给 node_B_1 的 pool1
* site_A-shelf_4 上的所有磁盘都会自动分配给 node_B_2 的 pool1


您必须通过` 在每个磁盘架上指定一个磁盘来 " 按 `s" 自动分配。

.步骤
. 在 MetroCluster IP 配置中的每个节点上，为池 1 分配一个远程磁盘。
+
.. 显示未分配磁盘的列表：
+
`disk show -host-adapter 0m -container-type unassigned`

+
[listing]
----
cluster_A::> disk show -host-adapter 0m -container-type unassigned
                     Usable           Disk    Container   Container
Disk                   Size Shelf Bay Type    Type        Name      Owner
---------------- ---------- ----- --- ------- ----------- --------- --------
6.23.0                    -    23   0 SSD     unassigned  -         -
6.23.1                    -    23   1 SSD     unassigned  -         -
.
.
.
node_A_2:0m.i1.2L51       -    21  14 SSD     unassigned  -         -
node_A_2:0m.i1.2L64       -    21  10 SSD     unassigned  -         -
.
.
.
48 entries were displayed.

cluster_A::>
----
.. 选择一个远程磁盘（ 0m ）并将该磁盘的所有权分配给第一个节点的池 1 （例如， "`node_A_1` " ）：
+
`ddisk assign -disk disk-id -pool 1 -owner owner-node-name`

+
`disk-id` 必须标识远程磁盘架 `owner-node-name` 上的磁盘。

+
ONTAP 磁盘自动分配功能可分配包含指定磁盘的远程磁盘架上的所有磁盘。

.. 至少等待 60 秒，以便执行磁盘自动分配后，验证磁盘架上的远程磁盘是否已自动分配到池 1 ：
+
`disk show -host-adapter 0m -container-type unassigned`

+
--

NOTE: 用于访问远程磁盘的 iSCSI 连接显示为设备 0m 。

--
+
以下输出显示磁盘架 "`23` " 上的磁盘已分配，不再显示：

+
[listing]
----
cluster_A::> disk show -host-adapter 0m -container-type unassigned
                     Usable           Disk    Container   Container
Disk                   Size Shelf Bay Type    Type        Name      Owner
---------------- ---------- ----- --- ------- ----------- --------- --------
node_A_2:0m.i1.2L51       -    21  14 SSD     unassigned  -         -
node_A_2:0m.i1.2L64       -    21  10 SSD     unassigned  -         -
node_A_2:0m.i1.2L72       -    21  23 SSD     unassigned  -         -
node_A_2:0m.i1.2L74       -    21   1 SSD     unassigned  -         -
node_A_2:0m.i1.2L83       -    21  22 SSD     unassigned  -         -
node_A_2:0m.i1.2L90       -    21   7 SSD     unassigned  -         -
node_A_2:0m.i1.3L52       -    21   6 SSD     unassigned  -         -
node_A_2:0m.i1.3L59       -    21  13 SSD     unassigned  -         -
node_A_2:0m.i1.3L66       -    21  17 SSD     unassigned  -         -
node_A_2:0m.i1.3L73       -    21  12 SSD     unassigned  -         -
node_A_2:0m.i1.3L80       -    21   5 SSD     unassigned  -         -
node_A_2:0m.i1.3L81       -    21   2 SSD     unassigned  -         -
node_A_2:0m.i1.3L82       -    21  16 SSD     unassigned  -         -
node_A_2:0m.i1.3L91       -    21   3 SSD     unassigned  -         -
node_A_2:0m.i2.0L49       -    21  15 SSD     unassigned  -         -
node_A_2:0m.i2.0L50       -    21   4 SSD     unassigned  -         -
node_A_2:0m.i2.1L57       -    21  18 SSD     unassigned  -         -
node_A_2:0m.i2.1L58       -    21  11 SSD     unassigned  -         -
node_A_2:0m.i2.1L59       -    21  21 SSD     unassigned  -         -
node_A_2:0m.i2.1L65       -    21  20 SSD     unassigned  -         -
node_A_2:0m.i2.1L72       -    21   9 SSD     unassigned  -         -
node_A_2:0m.i2.1L80       -    21   0 SSD     unassigned  -         -
node_A_2:0m.i2.1L88       -    21   8 SSD     unassigned  -         -
node_A_2:0m.i2.1L90       -    21  19 SSD     unassigned  -         -
24 entries were displayed.

cluster_A::>
----
.. 重复上述步骤，将池 1 磁盘分配给站点 A 上的第二个节点（例如， "`node_A_2` " ）。
.. 在站点 B 上重复这些步骤






=== 在 ONTAP 9.4 中启用驱动器自动分配

在 ONTAP 9.4 中，如果您按照先前在此操作步骤中的指示禁用了自动驱动器分配，则必须在所有节点上重新启用它。

link:concept_considerations_drive_assignment.html["ONTAP 9.4 及更高版本中的自动驱动器分配和 ADP 系统注意事项"]

.步骤
. 启用自动驱动器分配：
+
`s存储磁盘选项 modify -node _node_name_ -autodassign on`

+
您必须在 MetroCluster IP 配置中的所有节点上问题描述此命令。





=== 镜像根聚合

您必须镜像根聚合以提供数据保护。

默认情况下，根聚合创建为 RAID-DP 类型的聚合。您可以将根聚合从 RAID-DP 更改为 RAID4 类型的聚合。以下命令修改 RAID4 类型聚合的根聚合：

`storage aggregate modify – aggregate _aggr_name_ -RAIDType RAID4`


NOTE: 在非 ADP 系统上，可以在镜像聚合之前或之后将聚合的 RAID 类型从默认 RAID-DP 修改为 RAID4 。

.步骤
. 镜像根聚合：
+
`s存储聚合镜像 _aggr_name_`

+
以下命令镜像 controller_A_1 的根聚合：

+
[listing]
----
controller_A_1::> storage aggregate mirror aggr0_controller_A_1
----
+
此操作会镜像聚合，因此它包含一个本地丛和一个位于远程 MetroCluster 站点的远程丛。

. 对 MetroCluster 配置中的每个节点重复上述步骤。


https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-vsmg/home.html["逻辑存储管理"]



=== 在每个节点上创建镜像数据聚合

您必须在 DR 组中的每个节点上创建镜像数据聚合。

.关于此任务
* 您应了解新聚合将使用哪些驱动器。
* 如果系统中有多种驱动器类型（异构存储），则应了解如何确保选择正确的驱动器类型。
* 驱动器由特定节点拥有；创建聚合时，该聚合中的所有驱动器都必须由同一节点拥有，该节点将成为该聚合的主节点。
+
在使用 ADP 的系统中，聚合是使用分区创建的，其中每个驱动器都分区为 P1 ， P2 和 P3 分区。

* 聚合名称应符合您在规划 MetroCluster 配置时确定的命名方案。
+
https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-psmg/home.html["磁盘和聚合管理"]



.步骤
. 显示可用备件列表：
+
`storage disk show -spare -owner _node_name_`

. 创建聚合：
+
`storage aggregate create -mirror true`

+
如果您已通过集群管理界面登录到集群，则可以在集群中的任何节点上创建聚合。要确保在特定节点上创建聚合，请使用 ` -node` 参数或指定该节点所拥有的驱动器。

+
您可以指定以下选项：

+
** 聚合的主节点（即在正常操作下拥有聚合的节点）
** 要添加到聚合的特定驱动器的列表
** 要包含的驱动器数量
+

NOTE: 在支持的最低配置中，可用驱动器数量有限，您必须使用 force-Small-aggregate 选项来创建三磁盘 RAID-DP 聚合。

** 要用于聚合的校验和模式
** 要使用的驱动器类型
** 要使用的驱动器大小
** 要使用的驱动器速度
** 聚合上 RAID 组的 RAID 类型
** 可包含在 RAID 组中的最大驱动器数
** 是否允许使用 RPM 不同的驱动器
+
有关这些选项的详细信息，请参见 storage aggregate create 手册页。

+
以下命令将创建包含 10 个磁盘的镜像聚合：

+
[listing]
----
cluster_A::> storage aggregate create aggr1_node_A_1 -diskcount 10 -node node_A_1 -mirror true
[Job 15] Job is queued: Create aggr1_node_A_1.
[Job 15] The job is starting.
[Job 15] Job succeeded: DONE
----


. 验证新聚合的 RAID 组和驱动器：
+
`storage aggregate show-status -aggregate _aggregate-name_`





=== 实施 MetroCluster 配置

要在 MetroCluster 配置中启动数据保护，必须运行 `MetroCluster configure` 命令。

.关于此任务
* 每个集群上应至少有两个非根镜像数据聚合。
+
您可以使用 storage aggregate show 命令对此进行验证。

+

NOTE: 如果要使用单个镜像数据聚合，请参见 <<step1_single_mirrored,第 1 步>> 有关说明，请参见。

* 控制器和机箱的 ha-config 状态必须为 `mCCIP` 。


您可以在任何节点上问题描述一次 `MetroCluster configure` 命令，以启用 MetroCluster 配置。您无需在每个站点或节点上对命令执行问题描述，也无需选择对哪个节点或站点执行问题描述命令。

`MetroCluster configure` 命令会自动将两个集群中每个集群中系统 ID 最低的两个节点配对，作为灾难恢复（ DR ）配对节点。在四节点 MetroCluster 配置中，存在两个 DR 配对节点对。第二个 DR 对是从系统 ID 较高的两个节点创建的。

.步骤
. 【第 1 步 _single 或 mirrored]] 按照以下格式配置 MetroCluster ：
+
|===


| 如果您的 MetroCluster 配置 ... | 然后执行此操作 ... 


 a| 
多个数据聚合
 a| 
从任何节点的提示符处，配置 MetroCluster ： `MetroCluster configure node-name`



 a| 
一个镜像数据聚合
 a| 
.. 在任何节点的提示符处，更改为高级权限级别：
+
`set -privilege advanced`

+
当系统提示您继续进入高级模式且您看到高级模式提示符（` * >` ）时，您需要使用 "`y` " 进行响应。

.. 使用 ` -allow-with-one-aggregate` true 参数配置 MetroCluster ：
+
MetroCluster configure -allow-with-one-aggregate true node-name`

.. 返回到管理权限级别：
+
`set -privilege admin`



|===
+
--
[NOTE]
====
最佳实践是具有多个数据聚合。如果第一个 DR 组只有一个聚合，而您要添加一个具有一个聚合的 DR 组，则必须将元数据卷从单个数据聚合中移出。有关此操作步骤的详细信息，请参见 link:../maintain/task_move_a_metadata_volume_in_mcc_configurations.html["在 MetroCluster 配置中移动元数据卷"]。

====
--
+
以下命令将在包含 "`controller_A_1` " 的 DR 组中的所有节点上启用 MetroCluster 配置：

+
[listing]
----
cluster_A::*> metrocluster configure -node-name controller_A_1

[Job 121] Job succeeded: Configure is successful.
----
. 验证站点 A 上的网络连接状态：
+
`network port show`

+
以下示例显示了四节点 MetroCluster 配置中的网络端口使用情况：

+
[listing]
----
cluster_A::> network port show
                                                          Speed (Mbps)
Node   Port      IPspace   Broadcast Domain Link   MTU    Admin/Oper
------ --------- --------- ---------------- ----- ------- ------------
controller_A_1
       e0a       Cluster   Cluster          up     9000  auto/1000
       e0b       Cluster   Cluster          up     9000  auto/1000
       e0c       Default   Default          up     1500  auto/1000
       e0d       Default   Default          up     1500  auto/1000
       e0e       Default   Default          up     1500  auto/1000
       e0f       Default   Default          up     1500  auto/1000
       e0g       Default   Default          up     1500  auto/1000
controller_A_2
       e0a       Cluster   Cluster          up     9000  auto/1000
       e0b       Cluster   Cluster          up     9000  auto/1000
       e0c       Default   Default          up     1500  auto/1000
       e0d       Default   Default          up     1500  auto/1000
       e0e       Default   Default          up     1500  auto/1000
       e0f       Default   Default          up     1500  auto/1000
       e0g       Default   Default          up     1500  auto/1000
14 entries were displayed.
----
. 从 MetroCluster 配置中的两个站点验证 MetroCluster 配置。
+
.. 从站点 A 验证配置：
+
`MetroCluster show`

+
[listing]
----
cluster_A::> metrocluster show

Configuration: IP fabric

Cluster                   Entry Name          State
------------------------- ------------------- -----------
 Local: cluster_A         Configuration state configured
                          Mode                normal
Remote: cluster_B         Configuration state configured
                          Mode                normal
----
.. 从站点 B 验证配置：
+
`MetroCluster show`

+
[listing]
----
cluster_B::> metrocluster show

Configuration: IP fabric

Cluster                   Entry Name          State
------------------------- ------------------- -----------
 Local: cluster_B         Configuration state configured
                          Mode                normal
Remote: cluster_A         Configuration state configured
                          Mode                normal
----


. 为了避免非易失性内存镜像可能出现的问题，请重新启动四个节点中的每个节点：
+
`node reboot -node _node-name_ -inhibit-takeover true`

. 在两个集群上运行 `MetroCluster show` 命令以再次验证配置。问题描述




=== 在八节点配置中配置第二个 DR 组

重复上述任务以配置第二个 DR 组中的节点。



=== 创建未镜像的数据聚合

您可以选择为不需要 MetroCluster 配置提供的冗余镜像的数据创建未镜像数据聚合。

.关于此任务
* 您应了解新聚合将使用哪些驱动器或阵列 LUN 。
* 如果系统中有多种驱动器类型（异构存储），则应了解如何验证是否选择了正确的驱动器类型。



IMPORTANT: 在 MetroCluster IP 配置中，切换后无法访问未镜像的远程聚合


NOTE: 未镜像聚合必须位于其所属节点的本地。

* 驱动器和阵列 LUN 归特定节点所有；创建聚合时，该聚合中的所有驱动器都必须归同一节点所有，该节点将成为该聚合的主节点。
* 聚合名称应符合您在规划 MetroCluster 配置时确定的命名方案。
* _Disks and aggregates management_ 包含有关镜像聚合的详细信息。


.步骤
. 启用未镜像聚合部署：
+
MetroCluster modify -enable-unmirrored-aggr-deployment true`

. 验证是否已禁用磁盘自动分配：
+
`d` 选项显示

. 安装要包含未镜像聚合的磁盘架并为其布线。
+
您可以对平台和磁盘架使用 _Installation and Setup_ 文档中的过程。

+
https://docs.netapp.com/platstor/index.jsp["AFF 和 FAS 文档中心"]

. 手动将新磁盘架上的所有磁盘分配给相应的节点：
+
`ddisk assign -disk _disk-id_ -owner _owner-node-name_`

. 创建聚合：
+
`s存储聚合创建`

+
如果您已通过集群管理界面登录到集群，则可以在集群中的任何节点上创建聚合。要验证是否已在特定节点上创建聚合，应使用 ` -node` 参数或指定该节点所拥有的驱动器。

+
此外，还必须确保仅在聚合中包含未镜像磁盘架上的驱动器。

+
您可以指定以下选项：

+
** 聚合的主节点（即在正常操作下拥有聚合的节点）
** 要添加到聚合的特定驱动器或阵列 LUN 的列表
** 要包含的驱动器数量
** 要用于聚合的校验和模式
** 要使用的驱动器类型
** 要使用的驱动器大小
** 要使用的驱动器速度
** 聚合上 RAID 组的 RAID 类型
** 可包含在 RAID 组中的驱动器或阵列 LUN 的最大数量
** 是否允许使用 RPM 不同的驱动器
+
有关这些选项的详细信息，请参见 storage aggregate create 手册页。

+
以下命令将创建一个包含 10 个磁盘的未镜像聚合：

+
[listing]
----
controller_A_1::> storage aggregate create aggr1_controller_A_1 -diskcount 10 -node controller_A_1
[Job 15] Job is queued: Create aggr1_controller_A_1.
[Job 15] The job is starting.
[Job 15] Job succeeded: DONE
----


. 验证新聚合的 RAID 组和驱动器：
+
`storage aggregate show-status -aggregate _aggregate-name_`

. 禁用未镜像聚合部署：
+
MetroCluster modify -enable-unmirrored-aggr-deployment false`

. 验证是否已启用磁盘自动分配：
+
`d` 选项显示



https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-psmg/home.html["磁盘和聚合管理"^]



=== 正在检查 MetroCluster 配置

您可以检查 MetroCluster 配置中的组件和关系是否工作正常。您应在初始配置后以及对 MetroCluster 配置进行任何更改后执行检查。您还应在协商（计划内）切换或切回操作之前执行检查。

如果在任一集群或同时在这两个集群上短时间内发出 `MetroCluster check run` 命令两次，则可能发生冲突，并且此命令可能无法收集所有数据。后续的 `MetroCluster check show` 命令不会显示预期输出。

.步骤
. 检查配置：
+
`MetroCluster check run`

+
此命令作为后台作业运行，可能无法立即完成。

+
[listing]
----
cluster_A::> metrocluster check run
The operation has been started and is running in the background. Wait for
it to complete and run "metrocluster check show" to view the results. To
check the status of the running metrocluster check operation, use the command,
"metrocluster operation history show -job-id 2245"
----
+
[listing]
----
cluster_A::> metrocluster check show
Last Checked On: 9/13/2018 20:41:37

Component           Result
------------------- ---------
nodes               ok
lifs                ok
config-replication  ok
aggregates          ok
clusters            ok
connections         ok
6 entries were displayed.
----
. 显示最近的 `MetroCluster check run` 命令的更详细结果：
+
`MetroCluster check aggregate show`

+
`MetroCluster check cluster show`

+
`MetroCluster check config-replication show`

+
`MetroCluster check lif show`

+
`MetroCluster check node show`

+
`MetroCluster check show` 命令可显示最新的 `MetroCluster check run` 命令的结果。在使用 `MetroCluster check show` 命令之前，应始终运行 `MetroCluster check run` 命令，以使显示的信息为最新信息。

+
以下示例显示了运行正常的四节点 MetroCluster 配置的 `MetroCluster check aggregate show` 命令输出：

+
[listing]
----
cluster_A::> metrocluster check aggregate show

Last Checked On: 8/5/2014 00:42:58

Node                  Aggregate                  Check                      Result
---------------       --------------------       ---------------------      ---------
controller_A_1        controller_A_1_aggr0
                                                 mirroring-status           ok
                                                 disk-pool-allocation       ok
                                                 ownership-state            ok
                      controller_A_1_aggr1
                                                 mirroring-status           ok
                                                 disk-pool-allocation       ok
                                                 ownership-state            ok
                      controller_A_1_aggr2
                                                 mirroring-status           ok
                                                 disk-pool-allocation       ok
                                                 ownership-state            ok


controller_A_2        controller_A_2_aggr0
                                                 mirroring-status           ok
                                                 disk-pool-allocation       ok
                                                 ownership-state            ok
                      controller_A_2_aggr1
                                                 mirroring-status           ok
                                                 disk-pool-allocation       ok
                                                 ownership-state            ok
                      controller_A_2_aggr2
                                                 mirroring-status           ok
                                                 disk-pool-allocation       ok
                                                 ownership-state            ok

18 entries were displayed.
----
+
以下示例显示了运行正常的四节点 MetroCluster 配置的 `MetroCluster check cluster show` 命令输出。它表示集群已准备好在必要时执行协商切换。

+
[listing]
----
Last Checked On: 9/13/2017 20:47:04

Cluster               Check                           Result
--------------------- ------------------------------- ---------
mccint-fas9000-0102
                      negotiated-switchover-ready     not-applicable
                      switchback-ready                not-applicable
                      job-schedules                   ok
                      licenses                        ok
                      periodic-check-enabled          ok
mccint-fas9000-0304
                      negotiated-switchover-ready     not-applicable
                      switchback-ready                not-applicable
                      job-schedules                   ok
                      licenses                        ok
                      periodic-check-enabled          ok
10 entries were displayed.
----


https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-psmg/home.html["磁盘和聚合管理"^]

https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-nmg/home.html["网络和 LIF 管理"^]



=== 正在完成 ONTAP 配置

配置，启用和检查 MetroCluster 配置后，您可以根据需要添加其他 SVM ，网络接口和其他 ONTAP 功能，从而继续完成集群配置。



== 验证切换，修复和切回

您应验证 MetroCluster 配置的切换，修复和切回操作。

. 使用中的协商切换，修复和切回过程 https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-mcc-mgmt-dr/home.html["MetroCluster 管理和灾难恢复"]




== 配置 MetroCluster Tiebreaker 或 ONTAP 调解器软件

您可以从第三个站点下载并安装 MetroCluster Tiebreaker 软件，也可以从 ONTAP 调解器 ONTAP 9.7 开始安装。

您必须有一个可用的 Linux 主机，该主机可通过网络连接到 MetroCluster 配置中的两个集群。具体要求请参见 MetroCluster Tiebreaker 或 ONTAP 调解器文档。

如果要连接到现有 Tiebreaker 或 ONTAP 调解器实例，则需要 Tiebreaker 或调解器服务的用户名，密码和 IP 地址。

如果您必须安装新的 ONTAP 调解器实例，请按照说明安装和配置软件。

link:concept_mediator_requirements.html["为计划外自动切换配置 ONTAP 调解器服务"]

如果您必须安装 Tiebreaker 软件的新实例，请按照说明安装和配置该软件。

https://docs.netapp.com/ontap-9/topic/com.netapp.doc.hw-metrocluster-tiebreaker/home.html["MetroCluster Tiebreaker 软件安装和配置"]

您不能使用具有相同 MetroCluster 配置的 MetroCluster Tiebreaker 软件和 ONTAP 调解器。

link:concept_considerations_differences.html["使用 ONTAP 调解器或 MetroCluster Tiebreaker 的注意事项"]

.步骤
. 配置 ONTAP 调解器服务或 Tiebreaker 软件：
+
** 如果使用的是现有的 ONTAP 调解器实例，请将 ONTAP 调解器服务添加到 ONTAP ：
+
MetroCluster configuration-settings mediator add -mediate-address _ip-address-fo-medier-host_`

** 如果您使用的是 Tiebreaker 软件，请参见 Tiebreaker 文档。
+
https://docs.netapp.com/ontap-9/topic/com.netapp.doc.hw-metrocluster-tiebreaker/home.html["MetroCluster Tiebreaker 软件安装和配置"]







== 保护配置备份文件

您可以通过指定一个远程 URL （ HTTP 或 FTP ）来为集群配置备份文件提供额外保护，除了本地集群中的默认位置之外，还可以将配置备份文件上传到该远程 URL 。

.步骤
. 为配置备份文件设置远程目标的 URL ：
+
`s系统配置备份设置 modify _url-of-destination_s`

+
使用命令行界面进行集群管理在 _Manag管理 配置备份 _ 一节下包含追加信息。



https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-sag/home.html["系统管理"^]
